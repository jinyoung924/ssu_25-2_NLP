import os
import glob
import json
import re
import pandas as pd
import torch
from torch.utils.data import Dataset
from transformers import (
    PreTrainedTokenizerFast, 
    BartForConditionalGeneration, 
    Seq2SeqTrainingArguments, 
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback
)
from tqdm.auto import tqdm

# ==========================================
# 1. RTX 4090 ìµœì í™” ì„¤ì • (Configuration)
# ==========================================
CONFIG = {
    "model_name": "gogamza/kobart-summarization",
    "data_dir": "/data/jinagg/022.ìš”ì•½ë¬¸_ë°_ë ˆí¬íŠ¸_ìƒì„±_ë°ì´í„°", 
    "cache_dir": "./cache_data",    
    "output_dir": "./results",      
    "max_input_len": 512,           
    "max_target_len": 128,          
    
    # [4090 ìµœì í™”] ë°°ì¹˜ ì‚¬ì´ì¦ˆ UP
    "batch_size": 32,                
    "num_epochs": 5,                
    "learning_rate": 2e-5,          
    "num_workers": 0,               # Windows ì—ëŸ¬ ë°©ì§€
    "seed": 42,
    
    "warmup_steps": 1000,
    "max_grad_norm": 1.0,
    "early_stopping_patience": 3 
}

torch.manual_seed(CONFIG['seed'])
if not os.path.exists(CONFIG['cache_dir']):
    os.makedirs(CONFIG['cache_dir'])

# ==========================================
# 2. í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜
# ==========================================
def clean_text(text):
    if not isinstance(text, str): return ""
    text = re.sub(r'\([^)]+[=].+?[=]', '', text)
    text = re.sub(r'\([^)]+\) *[^)]+ ê¸°ì =', '', text)
    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', text)
    text = re.sub(r'\[(ì‚¬ì§„|ìë£Œ)(ì œê³µ)?=.*?\]', '', text)
    text = re.sub(r'ë¬´ë‹¨ ?ì „ì¬.*?ê¸ˆì§€', '', text)
    text = re.sub(r'â“’.*?reserved\.', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# ==========================================
# 3. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ==========================================
def load_and_preprocess(split_keyword):
    cache_file = os.path.join(CONFIG['cache_dir'], f"{split_keyword}.csv")
    
    if os.path.exists(cache_file):
        print(f"ğŸš€ ìºì‹œëœ ë°ì´í„° ë¡œë“œ ì¤‘ ({split_keyword}): {cache_file}")
        return pd.read_csv(cache_file).dropna()

    print(f"ğŸ“‚ ì›ë³¸ ë°ì´í„° íƒìƒ‰ ì¤‘... í‚¤ì›Œë“œ: '{split_keyword}'")
    search_pattern = os.path.join(CONFIG['data_dir'], "**", "*.json")
    all_json_files = glob.glob(search_pattern, recursive=True)
    target_files = [f for f in all_json_files if split_keyword in f]
    
    if not target_files:
        print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {split_keyword}")
        return pd.DataFrame()
    
    print(f"   -> {len(target_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
    data_list = []
    for f in tqdm(target_files):
        try:
            with open(f, 'r', encoding='utf-8') as file:
                data = json.load(file)
                if isinstance(data, list): data_list.extend(data)
                else: data_list.append(data)
        except: continue
            
    if not data_list: return pd.DataFrame()

    df = pd.DataFrame(data_list)
    try:
        if 'Meta(Refine)' in df.columns:
            meta = pd.json_normalize(df['Meta(Refine)'])
            if 'passage' in meta.columns: df['passage'] = meta['passage']
        
        if 'Annotation' in df.columns:
            df['summary1'] = df['Annotation'].apply(
                lambda x: x['summary1'] if isinstance(x, dict) else (x[0]['summary1'] if isinstance(x, list) and len(x)>0 else "")
            )
    except: pass

    if 'passage' in df.columns and 'summary1' in df.columns:
        final_df = df[['passage', 'summary1']].dropna()
        final_df = final_df[final_df['passage'].str.len() > 10]
        final_df.to_csv(cache_file, index=False)
        return final_df
    else:
        return pd.DataFrame()

# ==========================================
# 4. Dataset ì •ì˜
# ==========================================
class SummaryDataset(Dataset):
    def __init__(self, df, tokenizer):
        self.df = df
        self.tokenizer = tokenizer
    
    def __len__(self): return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        inputs = self.tokenizer(
            clean_text(str(row['passage'])),
            max_length=CONFIG['max_input_len'],
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        targets = self.tokenizer(
            text_target=clean_text(str(row['summary1'])),
            max_length=CONFIG['max_target_len'],
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        labels = targets['input_ids'].squeeze()
        labels[labels == self.tokenizer.pad_token_id] = -100
        return {
            "input_ids": inputs['input_ids'].squeeze(),
            "attention_mask": inputs['attention_mask'].squeeze(),
            "labels": labels
        }

# ==========================================
# 5. ë©”ì¸ ì‹¤í–‰
# ==========================================
if __name__ == "__main__":
    print(f"ğŸ”¥ RTX 4090 ìµœì í™” í•™ìŠµ ì‹œì‘ (Batch: {CONFIG['batch_size']}, bf16: True)")
    
    tokenizer = PreTrainedTokenizerFast.from_pretrained(CONFIG['model_name'])
    model = BartForConditionalGeneration.from_pretrained(CONFIG['model_name'])
    
    train_df = load_and_preprocess("Training")
    val_df = load_and_preprocess("Validation")
    
    if train_df.empty or val_df.empty:
        print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨"); exit()

    train_dataset = SummaryDataset(train_df, tokenizer)
    eval_dataset = SummaryDataset(val_df, tokenizer)
    
    training_args = Seq2SeqTrainingArguments(
        output_dir=CONFIG['output_dir'],
        overwrite_output_dir=True,
        do_train=True,
        do_eval=True,
        num_train_epochs=CONFIG['num_epochs'],
        
        # [4090 í•µì‹¬ ì„¤ì •]
        per_device_train_batch_size=CONFIG['batch_size'],
        per_device_eval_batch_size=CONFIG['batch_size'],
        gradient_accumulation_steps=1,
        bf16=True, # 4090 ê°€ì† í•µì‹¬
        fp16=False,
        
        # [ì†ë„ í•µì‹¬] í‰ê°€ëŠ” ì—í­ ë‹¨ìœ„ë¡œ
        eval_strategy="epoch", 
        save_strategy="epoch",
        
        learning_rate=CONFIG['learning_rate'],
        weight_decay=0.01,
        warmup_steps=CONFIG['warmup_steps'],
        max_grad_norm=CONFIG['max_grad_norm'],
        
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        predict_with_generate=True,
        generation_max_length=CONFIG['max_target_len'],
        dataloader_num_workers=CONFIG['num_workers'],
        logging_dir='./logs',
        logging_steps=50,
        report_to="none"
    )
    
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),
        callbacks=[EarlyStoppingCallback(early_stopping_patience=CONFIG['early_stopping_patience'])]
    )
    
    trainer.train(resume_from_checkpoint=True)
    
    model.save_pretrained(os.path.join(CONFIG['output_dir'], "final_model"))
    tokenizer.save_pretrained(os.path.join(CONFIG['output_dir'], "final_model"))
    print("âœ¨ í•™ìŠµ ì™„ë£Œ!")