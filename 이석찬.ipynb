{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c9f35ab-02e4-42f5-ba88-0c91a1e266e9",
   "metadata": {},
   "source": [
    "<h1>진행순서</h1>\n",
    "<h3 style=\"margin:0px\">\n",
    "  1. 데이터 전처리 → 2. KoBART 학습 → 3. 요약 모델 실행 (후처리 동시에 진행)\n",
    "</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f30d088-560b-479d-b751-ccf1dad7805a",
   "metadata": {},
   "source": [
    "<h2>ROUGE 모델 평가 지표</h2>\n",
    "<h4 style=\"margin:0px;\">\n",
    "  \"eval_loss\": 0.13988181948661804, \n",
    "  \"eval_rouge1\": 40.4069, \n",
    "  \"eval_rouge2\": 24.6301, \n",
    "  \"eval_rougeL\": 38.6044, \n",
    "  \"eval_rougeLsum\": 38.6401\n",
    "</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117372b5-51e4-43ce-975e-ed6b8d67c9e5",
   "metadata": {},
   "source": [
    "<h1>데이터 전처리 과정</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa68a7e-8dca-4e1c-806e-e7c9c63c59e5",
   "metadata": {},
   "source": [
    "데이터 전처리 + KOBART학습 과정 개발환경 -> python 3.12사용 <span style=\"color:red\">(python3.13사용시 라이브러리 호환 문제)</span>\n",
    "사용 라이브러리\n",
    "absl-py==2.3.1\n",
    "accelerate==1.11.0\n",
    "aiohappyeyeballs==2.6.1\n",
    "aiohttp==3.13.0\n",
    "aiosignal==1.4.0\n",
    "anyio==4.11.0\n",
    "attrs==25.4.0\n",
    "certifi==2025.10.5\n",
    "charset-normalizer==3.4.3\n",
    "click==8.3.0\n",
    "colorama==0.4.6\n",
    "datasets==4.2.0\n",
    "dill==0.4.0\n",
    "evaluate==0.4.6\n",
    "filelock==3.13.1\n",
    "frozenlist==1.8.0\n",
    "fsspec==2024.6.1\n",
    "h11==0.16.0\n",
    "httpcore==1.0.9\n",
    "httpx==0.28.1\n",
    "huggingface-hub==0.35.3\n",
    "idna==3.10\n",
    "Jinja2==3.1.4\n",
    "joblib==1.5.2\n",
    "MarkupSafe==2.1.5\n",
    "mpmath==1.3.0\n",
    "multidict==6.7.0\n",
    "multiprocess==0.70.16\n",
    "networkx==3.3\n",
    "nltk==3.9.2\n",
    "numpy==2.1.2\n",
    "packaging==25.0\n",
    "pandas==2.3.3\n",
    "pillow==11.0.0\n",
    "propcache==0.4.1\n",
    "psutil==7.1.0\n",
    "pyarrow==21.0.0\n",
    "python-dateutil==2.9.0.post0\n",
    "pytz==2025.2\n",
    "PyYAML==6.0.3\n",
    "regex==2025.9.18\n",
    "requests==2.32.5\n",
    "rouge_score==0.1.2\n",
    "safetensors==0.6.2\n",
    "sentencepiece==0.2.1\n",
    "setuptools==70.2.0\n",
    "six==1.17.0\n",
    "sniffio==1.3.1\n",
    "sympy==1.13.1\n",
    "tokenizers==0.22.1\n",
    "torch==2.5.1+cu121\n",
    "torchaudio==2.5.1+cu121\n",
    "torchvision==0.20.1+cu121\n",
    "tqdm==4.67.1\n",
    "transformers==4.57.1\n",
    "typing_extensions==4.12.2\n",
    "tzdata==2025.2\n",
    "urllib3==2.5.0\n",
    "xxhash==3.6.0\n",
    "yarl==1.22.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00031887-3912-45f6-8ca8-bd275a37e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json, re, unicodedata, hashlib\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_DIR = r\"전처리할 csv파일 경로 입력\"\n",
    "OUTPUT_CSV = r\"전처리한 csv파일 저장할 경로 입력\"\n",
    "\n",
    "#보일러플레이트/광고/기자서명 제거\n",
    "BOILER_PATTERNS = [\n",
    "    r\"\\(?사진=.+?\\)\", r\"\\[사진\\].*?$\", r\"\\[영상\\].*?$\", r\"\\[전문\\].*?$\",\n",
    "    r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\",  # 이메일\n",
    "    r\"https?://\\S+\", r\"www\\.\\S+\",\n",
    "    r\"[#＃][\\w가-힣]+\",  # 해시태그\n",
    "    r\"ⓒ.*?$\", r\"무단전재.*?금지\", r\"재배포.*?금지\", r\"※.*?$\",\n",
    "    r\"기사제보.*?$\", r\"네이버.*?구독\", r\"페이스북.*?팔로우\", r\"카카오.*?채널\",\n",
    "    r\"\\([가-힣A-Za-z ]*기자\\)\", r\"[가-힣A-Za-z]+ 기자\",  # 기자명\n",
    "]\n",
    "\n",
    "def nfkc(s: str) -> str:\n",
    "    return unicodedata.normalize(\"NFKC\", s or \"\")\n",
    "\n",
    "def strip_boiler(s: str) -> str:\n",
    "    s = s.replace(\"\\r\", \"\\n\")\n",
    "    for pat in BOILER_PATTERNS:\n",
    "        s = re.sub(pat, \" \", s, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    # 괄호 속 출처/캡션류 간단 제거\n",
    "    s = re.sub(r\"\\([^)]{0,40}(출처|자료|사진|영상|그래픽)[^)]{0,40}\\)\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def normalize_spaces(s: str) -> str:\n",
    "    # 연속 공백/탭 축소, 과도한 줄바꿈 축소\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def ensure_sentence_ending(s: str) -> str:\n",
    "    # 요약이 문장부호 없이 끝나는 것 보정\n",
    "    if s and s[-1] not in \".!?。…\":\n",
    "        s += \".\"\n",
    "    return s\n",
    "\n",
    "#정규화 진행. \n",
    "def normalize_text(t: str) -> str:\n",
    "    t = nfkc(t) #전각/반각, 조합문자, 기호 통일\n",
    "    t = strip_boiler(t) #이메일, URL, 기자명, 저작권문구 등 보일러플레이트 제거\n",
    "    t = normalize_spaces(t) #연속 공백, 줄바꿈 정리\n",
    "    return t\n",
    "\n",
    "# 길이+겹침 기반 스코어, 요약\n",
    "def jaccard(a_tokens, b_tokens):\n",
    "    A, B = set(a_tokens), set(b_tokens)\n",
    "    return len(A & B) / max(1, len(A | B))\n",
    "\n",
    "WORD_RE = re.compile(r\"[가-힣A-Za-z0-9]+\")\n",
    "\n",
    "def tokenize_simple(s: str):\n",
    "    return WORD_RE.findall(s)\n",
    "\n",
    "def length_score(n_tokens, low=25, high=120):  # 토큰 기준 가이드\n",
    "    if n_tokens <= 0: return 0.0\n",
    "    if n_tokens < low:\n",
    "        return n_tokens / low  # 짧을수록 점수 감소\n",
    "    if n_tokens > high:\n",
    "        # 길수록 감소, high*1.5 지점에서 0.5까지\n",
    "        return max(0.5, (high*1.5 - n_tokens) / (high*1.5 - high))\n",
    "    return 1.0\n",
    "\n",
    "def pick_summary_heuristic(d: dict, passage: str) -> str:\n",
    "    ann = d.get(\"Annotation\", {})\n",
    "    cands = [ann.get(\"summary1\"), ann.get(\"summary2\"), ann.get(\"summary3\")]\n",
    "    cands = [normalize_text(x) for x in cands if x]\n",
    "    if not cands:\n",
    "        return \"\"\n",
    "    p_tokens = tokenize_simple(passage)\n",
    "    best, best_score = \"\", -1.0\n",
    "    for c in cands:\n",
    "        c_tokens = tokenize_simple(c)\n",
    "        ls = length_score(len(c_tokens))\n",
    "        js = jaccard(set(c_tokens), set(p_tokens))\n",
    "        score = 0.6 * js + 0.4 * ls  # 가중치(겹침 60%, 길이 40%) — 데이터에 맞게 조절\n",
    "        if score > best_score:\n",
    "            best, best_score = c, score\n",
    "    return ensure_sentence_ending(best.strip())\n",
    "\n",
    "def extract_row(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    passage = normalize_text(data.get(\"Meta(Refine)\", {}).get(\"passage\", \"\"))\n",
    "    summary = pick_summary_heuristic(data, passage)\n",
    "\n",
    "    return {\n",
    "        \"article\": passage,\n",
    "        \"summary\": summary,\n",
    "        \"doc_id\": data.get(\"Meta(Acqusition)\", {}).get(\"doc_id\", os.path.basename(path))\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    files = sorted(glob.glob(os.path.join(INPUT_DIR, \"*.json\")))\n",
    "    rows = []\n",
    "    for fp in files:\n",
    "        try:\n",
    "            row = extract_row(fp)\n",
    "            art, summ = row[\"article\"], row[\"summary\"]\n",
    "            # 길이 필터 (문자 수 기준 + 토큰 비율 기준)\n",
    "            if len(art) < 80 or len(summ) < 30:\n",
    "                continue\n",
    "            a_tok, s_tok = tokenize_simple(art), tokenize_simple(summ)\n",
    "            if len(s_tok) < 15 or len(a_tok) < 80:\n",
    "                continue\n",
    "            ratio = len(a_tok) / max(1, len(s_tok))\n",
    "            if not (4.0 <= ratio <= 12.0):\n",
    "                continue\n",
    "            # 본문-요약 과도한 복붙 제거\n",
    "            if jaccard(a_tok, s_tok) > 0.85:\n",
    "                continue\n",
    "            rows.append(row)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] skip {fp}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"doc_id\", \"article\", \"summary\"])\n",
    "\n",
    "    # 중복/유사중복 제거\n",
    "    # 완전중복\n",
    "    df.drop_duplicates(subset=[\"article\", \"summary\"], inplace=True)\n",
    "\n",
    "    # 유사중복(해시 키 기반 빠른 걸러내기)\n",
    "    def fast_key(s):  # 문장부호 제거 후 해시\n",
    "        t = re.sub(r\"[^\\w가-힣]\", \"\", s)\n",
    "        return hashlib.md5(t.encode(\"utf-8\")).hexdigest()\n",
    "    df[\"a_key\"] = df[\"article\"].map(fast_key)\n",
    "    df[\"s_key\"] = df[\"summary\"].map(fast_key)\n",
    "    df.drop_duplicates(subset=[\"a_key\", \"s_key\"], inplace=True)\n",
    "    df.drop(columns=[\"a_key\", \"s_key\"], inplace=True)\n",
    "\n",
    "    #저장\n",
    "    df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved {len(df)} rows to {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d40a13-0d2b-4492-a780-b4b6683c1b77",
   "metadata": {},
   "source": [
    "<h1>KOBART학습 과정</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c2ac1-9e48-4d19-a204-50a62b921ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "입력방법 -> 해당 내용을 powershell에 입력\n",
    "python 파이썬 경로 입력 `\n",
    "  --train_csv \"학습 csv파일 경로 입력\" `\n",
    "  --val_csv   \"검증 csv파일 경로 입력\" `\n",
    "  --output_dir \"학습시킨 모델 저장할 경로 입력\" `\n",
    "  --------하이퍼 파라미터 값 지정---------\n",
    "  --epochs 3 `\n",
    "  --batch_size 2 `\n",
    "  --grad_accum 2 `\n",
    "  --max_input_len 1024 `\n",
    "  --max_target_len 128 `\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--train_csv\", type=str, required=True,\n",
    "                    help=\"학습 CSV (columns: article, summary)\")\n",
    "    ap.add_argument(\"--val_csv\",   type=str, required=True,\n",
    "                    help=\"검증 CSV (columns: article, summary)\")\n",
    "    ap.add_argument(\"--model_id\",  type=str, default=\"gogamza/kobart-summarization\")\n",
    "    ap.add_argument(\"--output_dir\", type=str, default=\"./kobart-news-sum\")\n",
    "    ap.add_argument(\"--epochs\", type=int, default=1)             # 학습 에폭\n",
    "    ap.add_argument(\"--batch_size\", type=int, default=2)         # 장치당 배치 크기\n",
    "    ap.add_argument(\"--grad_accum\", type=int, default=2)         # 그래디언트 누적 단계\n",
    "    ap.add_argument(\"--lr\", type=float, default=5e-5)\n",
    "    ap.add_argument(\"--warmup_ratio\", type=float, default=0.03)\n",
    "    ap.add_argument(\"--max_input_len\", type=int, default=1024)   # 입력 최대 토큰\n",
    "    ap.add_argument(\"--max_target_len\", type=int, default=128)   # 요약 최대 토큰\n",
    "    ap.add_argument(\"--logging_steps\", type=int, default=50)\n",
    "    ap.add_argument(\"--seed\", type=int, default=42)\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    # 디바이스/정밀도 자동 설정\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    use_fp16 = False\n",
    "    use_bf16 = False\n",
    "    if use_cuda:\n",
    "        use_fp16 = True   # Ampere(예: RTX 3080) 권장\n",
    "        use_bf16 = False\n",
    "        if args.batch_size == 1:\n",
    "            args.batch_size = 2\n",
    "        if args.grad_accum > 2:\n",
    "            args.grad_accum = 2\n",
    "\n",
    "    # 1) 데이터 로드\n",
    "    data_files = {\"train\": args.train_csv, \"validation\": args.val_csv}\n",
    "    raw = load_dataset(\"csv\", data_files=data_files)\n",
    "    dataset_train, dataset_val = raw[\"train\"], raw[\"validation\"]\n",
    "\n",
    "    # 2) 모델/토크나이저. KoBART 전용 SentencePiece 토크나이저 사용\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id, use_fast=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_id)\n",
    "\n",
    "    # 요약 태스크에선 분류 라벨 맵 불필요 → 불일치 무력화\n",
    "    try:\n",
    "        if hasattr(model.config, \"id2label\"):\n",
    "            model.config.num_labels = len(getattr(model.config, \"id2label\", {})) or 0\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # GPU 최적화 옵션\n",
    "    if use_cuda:\n",
    "        try:\n",
    "            model.gradient_checkpointing_enable()   # VRAM 절약\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            torch.set_float32_matmul_precision(\"high\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 전처리. KoBART의 SentencePiece 기반 토크나이저가 문장을 subword 단위(단어보다 작게, 자주 등장하는 음절/어절) 로 분리하고 라벨을 답니다.\n",
    "    def preprocess(batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        inputs = batch[\"article\"]\n",
    "        targets = batch[\"summary\"]\n",
    "\n",
    "        model_inputs = tokenizer(\n",
    "            inputs,\n",
    "            max_length=args.max_input_len,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        labels = tokenizer(\n",
    "            text_target=targets,\n",
    "            max_length=args.max_target_len,\n",
    "            truncation=True,\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "    cols_train = dataset_train.column_names\n",
    "    cols_val = dataset_val.column_names\n",
    "\n",
    "    tokenized_train = dataset_train.map(preprocess, batched=True, remove_columns=cols_train)\n",
    "    tokenized_val   = dataset_val.map(preprocess,   batched=True, remove_columns=cols_val)\n",
    "\n",
    "    # 데이터 콜레이터 \n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        pad_to_multiple_of=8 if use_fp16 else None,\n",
    "    )\n",
    "\n",
    "    # 평가 지표 (ROUGE)\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "        # labels의 -100 → pad token id 로 복구 후 디코딩\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        decoded_preds  = [p.strip() for p in decoded_preds]\n",
    "        decoded_labels = [l.strip() for l in decoded_labels]\n",
    "        result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "        # ROUGE 값은 0~1 → %로 변환\n",
    "        return {k: round(v * 100, 4) if k.startswith(\"rouge\") else v for k, v in result.items()}\n",
    "\n",
    "    # 반복 억제 기본값\n",
    "    model.config.no_repeat_ngram_size = 3\n",
    "    model.config.repetition_penalty = 1.1\n",
    "\n",
    "    # 학습 설정 \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_rougeL\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "\n",
    "        learning_rate=args.lr,\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        weight_decay=0.01,\n",
    "\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=1 if not use_cuda else min(4, args.batch_size * 2),\n",
    "        gradient_accumulation_steps=args.grad_accum,\n",
    "\n",
    "        num_train_epochs=args.epochs,\n",
    "        logging_steps=args.logging_steps,\n",
    "\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=args.max_target_len,\n",
    "\n",
    "        dataloader_pin_memory=use_cuda,\n",
    "        fp16=use_fp16,\n",
    "        bf16=use_bf16,\n",
    "        report_to=\"none\",\n",
    "\n",
    "        \n",
    "        max_grad_norm=1.0,\n",
    "    )\n",
    "\n",
    "    # 8) Trainer \n",
    "    base_kwargs = dict(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            processing_class=tokenizer,\n",
    "            **base_kwargs,\n",
    "        )\n",
    "    except TypeError:\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            tokenizer=tokenizer,\n",
    "            **base_kwargs,\n",
    "        )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(args.output_dir)\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "    print(f\"===> Finished. Model saved to: {args.output_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc4916-1665-4067-b08c-fd2eb8028065",
   "metadata": {},
   "source": [
    "<h1>후처리시 사용 함수들 정의</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e2f5b4-577e-436a-a6e6-3e393b949eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _normalize(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.replace(\"\\r\", \"\\n\")\n",
    "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
    "    t = re.sub(r\"\\s*\\n\\s*\", \"\\n\", t)\n",
    "    t = re.sub(r\"(\\.|\\?|\\!){2,}\", r\"\\1\", t)\n",
    "    t = re.sub(r\"\\s*([.,!?…])\", r\"\\1\", t)\n",
    "    t = re.sub(r\"([.,!?…])([^\\s])\", r\"\\1 \\2\", t)\n",
    "    return t.strip()\n",
    "\n",
    "COMMON_FIXES = [\n",
    "    (r\"\\b여 학생\\b\", \"여학생\"),\n",
    "    (r\"\\b줄어 들\\b\", \"줄어들\"),\n",
    "    (r\"\\b늘어 날\\b\", \"늘어날\"),\n",
    "    (r\"\\s*%\\s*\", \"%\"),\n",
    "]\n",
    "\n",
    "def _apply_common_fixes(text: str) -> str:\n",
    "    t = text\n",
    "    for pat, rep in COMMON_FIXES:\n",
    "        t = re.sub(pat, rep, t)\n",
    "    return t\n",
    "\n",
    "ENDINGS_RULES = [\n",
    "    (r\"(로|으로|에|에서|에게|과|와|및|의|를|을|은|는|가|이)[\\.\\u2026]*$\", \" 나타났다.\"),\n",
    "    (r\"(중이다|중|경향이다|경향|추세|상태)[\\.\\u2026]*$\", \"이다.\"),\n",
    "    (r\"(증가|감소|확대|축소|상승|하락)[\\.\\u2026]*$\", \"했다.\"),\n",
    "    (r\"(필요|예정|전망)[\\.\\u2026]*$\", \"이다.\"),\n",
    "    (r\"(한|함)[\\.\\u2026]*$\", \" 것으로 보인다.\"),\n",
    "]\n",
    "\n",
    "PHRASE_MAP = {\n",
    "    \"꾸준한.\": \"꾸준한 증가세를 보였다.\",\n",
    "    \"확대.\": \"확대되는 추세다.\",\n",
    "    \"감소.\": \"감소하는 모습을 보였다.\",\n",
    "}\n",
    "\n",
    "def _smart_finalize(sentence: str) -> str:\n",
    "    s = sentence.strip()\n",
    "    if not s:\n",
    "        return s\n",
    "    for k, v in PHRASE_MAP.items():\n",
    "        if s.endswith(k):\n",
    "            return s[:-len(k)] + v\n",
    "    for pat, tail in ENDINGS_RULES:\n",
    "        if re.search(pat, s):\n",
    "            s = re.sub(pat, tail, s)\n",
    "            break\n",
    "    if s and s[-1] not in \".!?…。\":\n",
    "        s += \".\"\n",
    "    return s\n",
    "\n",
    "def polish_korean_summary(text: str) -> str:\n",
    "    t = _normalize(text)\n",
    "    t = _apply_common_fixes(t)\n",
    "    parts = re.split(r\"(?<=[\\.!?…。])\\s+\", t) if t else []\n",
    "    if parts:\n",
    "        parts[-1] = _smart_finalize(parts[-1])\n",
    "    out = \" \".join([p for p in parts if p])\n",
    "    out = _normalize(out)\n",
    "    return out\n",
    "\n",
    "__all__ = [\"polish_korean_summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d687e-7f69-48a3-938f-1400d53ba849",
   "metadata": {},
   "source": [
    "<h1>요약 모델 실행 과정</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf096b-e51d-41dd-917a-aa3efe6dd4d5",
   "metadata": {},
   "source": [
    "요약 모델 실행 과정 개발환경 -> python3.13 사용\n",
    "사용 라이브러리\n",
    "certifi==2025.10.5\n",
    "charset-normalizer==3.4.4\n",
    "colorama==0.4.6\n",
    "filelock==3.20.0\n",
    "fsspec==2025.10.0\n",
    "huggingface-hub==0.36.0\n",
    "idna==3.11\n",
    "Jinja2==3.1.6\n",
    "MarkupSafe==3.0.3\n",
    "mpmath==1.3.0\n",
    "networkx==3.5\n",
    "numpy==2.3.4\n",
    "packaging==25.0\n",
    "pandas==2.3.3\n",
    "python-dateutil==2.9.0.post0\n",
    "pytz==2025.2\n",
    "PyYAML==6.0.3\n",
    "regex==2025.10.23\n",
    "requests==2.32.5\n",
    "safetensors==0.6.2\n",
    "setuptools==80.9.0\n",
    "six==1.17.0\n",
    "sympy==1.14.0\n",
    "tokenizers==0.22.1\n",
    "torch==2.9.0\n",
    "tqdm==4.67.1\n",
    "transformers==4.57.1\n",
    "typing_extensions==4.15.0\n",
    "tzdata==2025.2\n",
    "urllib3==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a75a46-de0d-4278-b040-1d07a025fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "csv파일 입력 예식\n",
    "PowerShell에 입력:\n",
    "& 파이썬 파일 경로 입력 `\n",
    "  C:실행파일 경로 입력 `\n",
    "  --in  \"입력 csv경로 입력\" `\n",
    "  --out \"기사 요약한 csv파일 저장 경로 입력\" `\n",
    "  --text-col article <- 기사 원문이 묶여있는 column입력\n",
    "\"\"\"\n",
    "\n",
    "#(CUDA 결정론) torch import 전에 설정\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig\n",
    "\n",
    "# 재현성 고정\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cuda.matmul.allow_tf32 = False\n",
    "    torch.backends.cudnn.allow_tf32 = False\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "# (선택) 한국어 후처리 모듈\n",
    "try:\n",
    "    from ko_postprocess import polish_korean_summary\n",
    "except ImportError:\n",
    "    polish_korean_summary = lambda x: x\n",
    "\n",
    "# 모델 로드 & 설정 (num_labels/id2label 경고 원천 차단)\n",
    "MODEL_DIR = Path(r\"모델 경로 입력\").resolve()\n",
    "\n",
    "def _clean_config_json_on_disk(model_dir: Path):\n",
    "    \"\"\"모델 폴더의 config.json에서 분류 관련 잔여 키 제거 (경고 발생 원천 차단).\"\"\"\n",
    "    cfg_path = model_dir / \"config.json\"\n",
    "    if not cfg_path.exists():\n",
    "        return\n",
    "    try:\n",
    "        with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = json.load(f)\n",
    "    except Exception:\n",
    "        return\n",
    "    changed = False\n",
    "    for k in [\"id2label\", \"label2id\", \"num_labels\", \"problem_type\"]:\n",
    "        if k in cfg:\n",
    "            del cfg[k]\n",
    "            changed = True\n",
    "    if changed:\n",
    "        with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cfg, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "_clean_config_json_on_disk(MODEL_DIR)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_DIR.as_posix(), local_files_only=True, use_fast=True)\n",
    "cfg = AutoConfig.from_pretrained(MODEL_DIR.as_posix(), local_files_only=True)\n",
    "cfg.id2label, cfg.label2id, cfg.num_labels = {}, {}, 0\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_DIR.as_posix(),\n",
    "    local_files_only=True,\n",
    "    config=cfg,\n",
    ")\n",
    "model.config.id2label, model.config.label2id, model.config.num_labels = {}, {}, 0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device).eval()\n",
    "\n",
    "def _postprocess(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    if text and text[-1] not in \".!?…。\\\"'”’\":\n",
    "        text += \".\"\n",
    "    return text\n",
    "\n",
    "\n",
    "#가이드/프롬프트\n",
    "CATEGORY_HINTS = {\n",
    "    \"news_incident\": [\"사망\", \"부상\", \"심장마비\", \"화재\", \"사고\", \"폭발\", \"체포\", \"피해\", \"응급\", \"이송\", \"병원\"],\n",
    "    \"sports\":        [\"전반전\", \"후반전\", \"득점\", \"골\", \"어시스트\", \"승리\", \"패배\", \"라운드\", \"엘클라시코\", \"경기\"],\n",
    "    \"policy\":        [\"발표\", \"정책\", \"대책\", \"계획\", \"공표\", \"시행\", \"개정\", \"대통령\", \"장관\", \"국회\"],\n",
    "    \"market\":        [\"실적\", \"전망\", \"매출\", \"영업이익\", \"주가\", \"물가\", \"금리\", \"환율\", \"성장률\", \"지표\"],\n",
    "    \"reaction\":      [\"논란\", \"비판\", \"반응\", \"찬반\", \"여론\", \"논의\", \"논평\", \"입장\", \"평가\"],\n",
    "}\n",
    "\n",
    "COMMON_GUIDE = (\n",
    "    \"※ 아래 글의 핵심 내용을 2~4문장으로 요약하라.\\n\"\n",
    "    \"※ 핵심 사실(누가, 무엇을, 언제, 어디서, 왜, 어떻게)을 중심으로 정리하라.\\n\"\n",
    "    \"※ 새 정보는 추가하지 말고 의미를 유지하며 자연스럽게 재구성하라.\\n\"\n",
    ")\n",
    "\n",
    "CATEGORY_GUIDES = {\n",
    "    \"news_incident\": \"※ 사건의 원인·결과·조치를 중심으로 요약하라.\\n\",\n",
    "    \"sports\": \"※ 경기 결과·주요 장면을 요약하라.\\n\",\n",
    "    \"policy\": \"※ 정책·발표의 주요 내용을 요약하라.\\n\",\n",
    "    \"market\": \"※ 경제 지표나 실적의 변화를 중심으로 요약하라.\\n\",\n",
    "    \"reaction\": \"※ 논란·여론의 쟁점을 공정하게 요약하라.\\n\",\n",
    "}\n",
    "\n",
    "def detect_category(text: str) -> str:\n",
    "    scores = {k: sum(kw in text for kw in kws) for k, kws in CATEGORY_HINTS.items()}\n",
    "    best = max(scores, key=scores.get)\n",
    "    return best if scores[best] > 0 else None\n",
    "\n",
    "def build_guided_input(article: str) -> str:\n",
    "    cat = detect_category(article)\n",
    "    return COMMON_GUIDE + CATEGORY_GUIDES.get(cat, \"\") + article\n",
    "\n",
    "# 요약 (결정론: 샘플링 X, 빔서치)\n",
    "def summarize(text: str) -> str:\n",
    "    inputs = tok(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    # 토큰 id 안전성 체크\n",
    "    vocab_size = getattr(model.config, \"vocab_size\", None)\n",
    "    max_id = int(inputs[\"input_ids\"].max())\n",
    "    if vocab_size is not None and max_id >= vocab_size:\n",
    "        raise ValueError(f\"Token id {max_id} >= vocab_size {vocab_size}. MODEL_DIR 확인.\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,           # 결정론\n",
    "            num_beams=5,               # 빔서치\n",
    "            min_new_tokens=40,\n",
    "            max_new_tokens=150,\n",
    "            length_penalty=1.0,\n",
    "            no_repeat_ngram_size=4,\n",
    "            repetition_penalty=1.2,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "        )\n",
    "    return _postprocess(tok.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "# 후처리 (비문/중복 정리 -> 문장수 제한 순서!)\n",
    "def _norm_sent(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.replace(\"'\", \"\").replace('\"', \"\")\n",
    "    s = re.sub(r\"[‘’“”]\", \"\", s)\n",
    "    return s.strip().lower()\n",
    "\n",
    "def dedupe_similar_sentences(text: str, sim_threshold: float = 0.8) -> str:\n",
    "    sents = [s for s in re.split(r\"(?<=[\\.!?…。])\\s+\", text.strip()) if s]\n",
    "    kept, norm_tokens = [], []\n",
    "\n",
    "    def tokens(x): return set(_norm_sent(x).split())\n",
    "\n",
    "    for s in sents:\n",
    "        tk = tokens(s)\n",
    "        if not tk:\n",
    "            continue\n",
    "        dup = False\n",
    "        for prev_tk in norm_tokens:\n",
    "            inter = len(tk & prev_tk)\n",
    "            union = len(tk | prev_tk) or 1\n",
    "            jacc = inter / union\n",
    "            if jacc >= sim_threshold:\n",
    "                dup = True\n",
    "                break\n",
    "        if not dup:\n",
    "            kept.append(s)\n",
    "            norm_tokens.append(tk)\n",
    "    return \" \".join(kept).strip()\n",
    "\n",
    "def tidy_korean_summary(text: str) -> str:\n",
    "    t = text\n",
    "    t = t.replace(\"||\", \" \")\n",
    "    t = re.sub(r\"[‘’]\", \"'\", t)\n",
    "    t = re.sub(r'[“”]', '\"', t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    # [출처] 태그 중복 제거\n",
    "    m = re.match(r\"^\\[([^\\[\\]]{1,30})\\]\\s*\", t)\n",
    "    if m:\n",
    "        tag = m.group(0)\n",
    "        body = t[len(tag):]\n",
    "        body = re.sub(r\"\\[\\s*\"+re.escape(m.group(1))+r\"\\s*\\]\\s*\", \"\", body)\n",
    "        t = (tag + body).strip()\n",
    "\n",
    "    # 마침표/따옴표 정리\n",
    "    t = re.sub(r\"\\.\\s*\\.\", \".\", t)\n",
    "    t = re.sub(r'\"\\s*\"', '\"', t)\n",
    "\n",
    "    # 문장 분리\n",
    "    sents = [s.strip() for s in re.split(r\"(?<=[\\.!?…。])\\s+\", t) if s.strip()]\n",
    "\n",
    "    #비문/토막/명사형 종결 제거 \n",
    "    BAD_ENDINGS = (\n",
    "        \"은.\", \"는.\", \"이.\", \"가.\", \"를.\", \"을.\", \"로.\", \"과.\", \"와.\", \"에.\", \"의.\",\n",
    "        \"인.\",  # ← 문제였던 케이스\n",
    "        \"중.\", \"등.\", \"및.\", \"또.\", \"며.\", \"듯.\", \"만.\", \"뿐.\"\n",
    "    )\n",
    "    # 한국어 서술 종결(대략) 판정: ‘…다.’ 계열/의문·감탄/따옴표로 끝나는 문장 허용\n",
    "    VERB_END = re.compile(r\"(다|했다|됐다|였다|잇다|있다|없다|밝혔다|전했다|강조했다|열렸다|발표했다|추진한다)\\.$\")\n",
    "\n",
    "    clean = []\n",
    "    for s in sents:\n",
    "        s_norm = re.sub(r\"\\s+\", \"\", s)\n",
    "\n",
    "        # 너무 짧은 문장 제거\n",
    "        if len(s_norm) < 8:\n",
    "            continue\n",
    "\n",
    "        # 조사·명사형으로 끝나는 문장 제거\n",
    "        if any(s_norm.endswith(be) for be in BAD_ENDINGS):\n",
    "            continue\n",
    "\n",
    "        # 서술형 종결이 아닌 경우(명사형 종결 추정) 제거\n",
    "        if not (VERB_END.search(s) or s_norm.endswith((\"?\", \"!\", \"….\"))):\n",
    "            continue\n",
    "\n",
    "        clean.append(s)\n",
    "\n",
    "    # 마지막 문장 재검사(혹시 남아있으면 한 번 더 컷)\n",
    "    if clean and any(clean[-1].strip().endswith(be) for be in BAD_ENDINGS):\n",
    "        clean = clean[:-1]\n",
    "\n",
    "    t = \" \".join(clean).strip()\n",
    "\n",
    "    # 따옴표 짝수 보정\n",
    "    if t.count('\"') % 2 == 1:\n",
    "        t = t.replace('\"', '')\n",
    "    if t.count(\"'\") % 2 == 1:\n",
    "        t = t.replace(\"'\", \"\")\n",
    "\n",
    "    if t and t[-1] not in \".!?…。\\\"'”’\":\n",
    "        t += \".\"\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "def minimal_cleanup(summary: str, keep: int = 4) -> str:\n",
    "    \"\"\"마지막에 문장수만 제한.\"\"\"\n",
    "    text = summary.strip()\n",
    "    sents = [s for s in re.split(r\"(?<=[\\.!?…。])\\s+\", text) if s]\n",
    "    text = \" \".join(sents[:keep]).strip()\n",
    "    return text\n",
    "\n",
    "def summarize_article(article: str, *, keep_sents: int = 4) -> str:\n",
    "    guided = build_guided_input(article)\n",
    "    raw = summarize(guided)\n",
    "    cleaned = tidy_korean_summary(raw)                      # 1) 정리\n",
    "    cleaned = dedupe_similar_sentences(cleaned, 0.8)        # 2) 중복 제거\n",
    "    cleaned = minimal_cleanup(cleaned, keep=keep_sents)     # 3) 문장 수 제한\n",
    "    try:\n",
    "        cleaned = polish_korean_summary(cleaned)            # 4) (선택) 후광택\n",
    "    except Exception:\n",
    "        pass\n",
    "    return cleaned\n",
    "\n",
    "# CSV/XLSX/TSV 일괄 요약\n",
    "def _read_table_auto(path: str, encoding: str = \"utf-8\") -> pd.DataFrame:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".xlsx\", \".xls\"]:\n",
    "        try:\n",
    "            return pd.read_excel(path)          # openpyxl 필요\n",
    "        except Exception:\n",
    "            return pd.read_excel(path, engine=\"openpyxl\")\n",
    "    elif ext in [\".tsv\", \".tab\"]:\n",
    "        return pd.read_csv(path, sep=\"\\t\", encoding=encoding)\n",
    "    else:\n",
    "        return pd.read_csv(path, encoding=encoding)\n",
    "\n",
    "def _summarize_series(series, *, keep_sents=4):\n",
    "    results = []\n",
    "    series = series.fillna(\"\").astype(str)\n",
    "    for text in tqdm(series, desc=\"Summarizing\", ncols=100):\n",
    "        if not text.strip():\n",
    "            results.append(\"\")\n",
    "            continue\n",
    "        try:\n",
    "            s = summarize_article(text, keep_sents=keep_sents)\n",
    "        except Exception as e:\n",
    "            s = f\"[ERROR] {type(e).__name__}: {e}\"\n",
    "        results.append(s)\n",
    "    return results\n",
    "\n",
    "def summarize_csv(\n",
    "    input_path: str,\n",
    "    output_csv: str,\n",
    "    *,\n",
    "    text_col: str = \"article\",\n",
    "    keep_other_cols: bool = True,\n",
    "    keep_sents: int = 4,\n",
    "    encoding: str = \"utf-8\",\n",
    "):\n",
    "    df = _read_table_auto(input_path, encoding=encoding)\n",
    "    if text_col not in df.columns:\n",
    "        raise ValueError(f\"'{text_col}' 컬럼을 찾을 수 없습니다. 컬럼: {list(df.columns)}\")\n",
    "    summaries = _summarize_series(df[text_col], keep_sents=keep_sents)\n",
    "    if keep_other_cols:\n",
    "        df_out = df.copy()\n",
    "        df_out[\"summary\"] = summaries\n",
    "    else:\n",
    "        df_out = pd.DataFrame({\"summary\": summaries})\n",
    "    df_out.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    return output_csv\n",
    "\n",
    "# 메인: CLI 또는 단일 데모\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse, sys\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"KoBART 뉴스 요약 (결정론)\")\n",
    "    parser.add_argument(\"--in\", dest=\"input_path\", type=str, help=\"입력 경로 (xlsx/csv/tsv)\")\n",
    "    parser.add_argument(\"--out\", dest=\"output_csv\", type=str, help=\"출력 CSV 경로\")\n",
    "    parser.add_argument(\"--text-col\", dest=\"text_col\", type=str, default=\"article\", help=\"기사 본문 컬럼명 (기본: article)\")\n",
    "    parser.add_argument(\"--keep-sents\", dest=\"keep_sents\", type=int, default=4, help=\"최종 문장 수 (기본: 4)\")\n",
    "    parser.add_argument(\"--encoding\", dest=\"encoding\", type=str, default=\"utf-8\", help=\"입력 텍스트 인코딩(csv/tsv)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.input_path and args.output_csv:\n",
    "        saved = summarize_csv(\n",
    "            args.input_path,\n",
    "            args.output_csv,\n",
    "            text_col=args.text_col,\n",
    "            keep_sents=args.keep_sents,\n",
    "            encoding=args.encoding,\n",
    "        )\n",
    "        print(f\"[완료] 저장: {saved}\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    # 인자 없이 실행되면 단일 데모 (CSV/배치와 동일 파이프라인)\n",
    "    demo_article = \"\"\"기사본문 입력\"\"\"\n",
    "    print(\"\\n[요약 결과]\\n\", summarize_article(demo_article, keep_sents=4))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
