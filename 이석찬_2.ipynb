{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5085b5-58f9-4efa-87db-f69fdf602f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig\n",
    "import sys\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 유틸: 후처리 (tidy + drop incomplete)\n",
    "# ----------------------------\n",
    "def tidy_korean_summary(text: str) -> str:\n",
    "    t = re.sub(r\"\\s+\", \" \", (text or \"\")).strip()\n",
    "    if t and t[-1] not in \".!?…。\\\"'”’\":\n",
    "        t += \".\"\n",
    "    return t\n",
    "\n",
    "def drop_incomplete_sentences(text: str) -> str:\n",
    "    if not text or str(text).strip() == \"\":\n",
    "        return \"\"\n",
    "    sentences = re.split(r'(?<=[\\.\\?\\!。\\!?])\\s*', text)\n",
    "    sentences = [s.strip() for s in sentences if s and s.strip()]\n",
    "    if len(sentences) == 0:\n",
    "        return \"\"\n",
    "    if len(sentences) == 1:\n",
    "        last = sentences[0]\n",
    "        if len(last.split()) < 3 or re.search(r'(며$|고$|하며$|으로$|에게$|서$|로$|에$|에 대해$|한다$|된다$)', last):\n",
    "            return \"\"\n",
    "        return last\n",
    "    last = sentences[-1]\n",
    "    if (len(last.split()) < 4) or (not re.search(r'[\\.\\?\\!。\\!?:;]$', last)) or re.search(r'(며$|고$|하며$|으로$|에게$|서$|로$|에$|에 대해$|한다$|된다$|것이다$)', last):\n",
    "        sentences = sentences[:-1]\n",
    "    cleaned = \" \".join(sentences).strip()\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 무조건 kobart_finetuned 사용\n",
    "# ----------------------------\n",
    "def load_tokenizer_model(local_dir: str = \"./kobart_finetuned\"):\n",
    "    p = Path(local_dir)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"[ERROR] 로컬 모델 폴더를 찾을 수 없습니다: {local_dir}\\n\"\n",
    "            \"확인해주세요. (model.safetensors / tokenizer 파일들이 있어야 함)\"\n",
    "        )\n",
    "\n",
    "    print(f\"[INFO] Using ONLY local model: {local_dir}\")\n",
    "\n",
    "    # config.json 수정: num_labels 같은 classification 필드 제거\n",
    "    cfg_path = p / \"config.json\"\n",
    "    try:\n",
    "        if cfg_path.exists():\n",
    "            import json\n",
    "            with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                cfg = json.load(f)\n",
    "            for k in [\"id2label\", \"label2id\", \"num_labels\", \"problem_type\"]:\n",
    "                cfg.pop(k, None)\n",
    "            with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(cfg, f, ensure_ascii=False, indent=2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    cfg = AutoConfig.from_pretrained(local_dir, local_files_only=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_dir, use_fast=True, local_files_only=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(local_dir, config=cfg, local_files_only=True)\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 요약 호출 함수\n",
    "# ----------------------------\n",
    "def summarize_text(tokenizer, model, device, text: str, min_new_tokens: int, max_new_tokens: int) -> str:\n",
    "    if not text or str(text).strip() == \"\":\n",
    "        return \"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            num_beams=5,\n",
    "            min_new_tokens=min_new_tokens,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            no_repeat_ngram_size=3,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "    decoded = tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
    "    cleaned = tidy_korean_summary(decoded)\n",
    "    cleaned = drop_incomplete_sentences(cleaned)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# CSV/Excel 자동 읽기\n",
    "# ----------------------------\n",
    "def _read_table_auto(path: str, encoding: str = \"utf-8\"):\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".xlsx\", \".xls\"]:\n",
    "        return pd.read_excel(path, engine=\"openpyxl\")\n",
    "    elif ext in [\".tsv\", \".tab\"]:\n",
    "        return pd.read_csv(path, sep=\"\\t\", encoding=encoding)\n",
    "    else:\n",
    "        return pd.read_csv(path, encoding=encoding)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 메인 실행부\n",
    "# ----------------------------\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--in\", dest=\"input_path\", required=True)\n",
    "    ap.add_argument(\"--out\", dest=\"output_path\", required=True)\n",
    "    ap.add_argument(\"--text-col\", dest=\"text_col\", default=\"newsContent\")\n",
    "    ap.add_argument(\"--summary-col\", dest=\"summary_col\", default=\"summary\")\n",
    "    ap.add_argument(\"--min-tokens\", dest=\"min_tokens\", type=int, default=40)\n",
    "    ap.add_argument(\"--max-tokens\", dest=\"max_tokens\", type=int, default=150)\n",
    "    ap.add_argument(\"--encoding\", dest=\"encoding\", default=\"utf-8\")\n",
    "    ap.add_argument(\"--device\", dest=\"device\", default=None)\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    df = _read_table_auto(args.input_path, encoding=args.encoding)\n",
    "\n",
    "    tokenizer, model = load_tokenizer_model(\"./kobart_finetuned\")\n",
    "    device = args.device if args.device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(f\"[DEVICE] Using device: {device}\")\n",
    "\n",
    "\n",
    "    model.to(device).eval()\n",
    "\n",
    "    print(f\"[RUN] Summarizing '{args.text_col}' → '{args.summary_col}'\")\n",
    "\n",
    "    results = []\n",
    "    for text in tqdm(df[args.text_col].fillna(\"\").astype(str), desc=\"Summarizing\", ncols=120):\n",
    "        results.append(summarize_text(tokenizer, model, device, text, args.min_tokens, args.max_tokens))\n",
    "\n",
    "    df_out = df.copy()\n",
    "    df_out[args.summary_col] = results\n",
    "    df_out.to_csv(args.output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"[DONE] 요약 완료 → {args.output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "# 단일 문자열 요약 (파일 입출력 X)\n",
    "# ----------------------------\n",
    "def summarize_article(text: str) -> str:\n",
    "    \"\"\"\n",
    "    기사 본문 문자열(text)을 입력받아 kobart_finetuned 모델로 요약문을 반환합니다.\n",
    "    \"\"\"\n",
    "    model_dir = \"./kobart_finetuned\"\n",
    "    tokenizer, model = load_tokenizer_model(model_dir)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device).eval()\n",
    "\n",
    "    summary = summarize_text(\n",
    "        tokenizer, model, device,\n",
    "        text,\n",
    "        min_new_tokens=40,\n",
    "        max_new_tokens=150\n",
    "    )\n",
    "    return summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if \"--in\" in sys.argv and \"--out\" in sys.argv:\n",
    "        main()\n",
    "    \n",
    "    else:        \n",
    "        text = \"\"\"삼성전자가 2025년 3분기 실적을 발표했다. \n",
    "        영업이익은 전년 동기 대비 30% 증가하며 반도체 부문이 실적을 견인했다. \n",
    "        특히 AI 반도체 수요 확대가 주요 원인으로 분석된다.\"\"\"\n",
    "        result = summarize_article(text)\n",
    "        print(\"요약문:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
