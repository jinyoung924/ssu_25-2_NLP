{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09279ecb-b03e-444d-9e00-cb07faaf415e",
   "metadata": {},
   "source": [
    "<h2>1. kobart_finetuned 저장하고, 2. 아래 빨간색 글자대로 경로만 바꿔주고, 3. 위에서 아래로 순서대로 실행하시면 됩니다!</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e9fce-3663-4fcd-afe1-e2c1e4797dcd",
   "metadata": {},
   "source": [
    "<h2>필요한 Library import하기</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "69fc9aad-568d-4e5f-9107-da4447aad482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9364f-e8fd-4411-9275-93b8459e1b87",
   "metadata": {},
   "source": [
    "<h2>모델 호출 함수 정의</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0058017f-dd4b-4949-a2c5-aac5437aa68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_model(local_dir: str):\n",
    "    p = Path(local_dir)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"[ERROR] 로컬 모델 폴더를 찾을 수 없습니다: {local_dir}\\n\"\n",
    "            \"확인해주세요. (model.safetensors / tokenizer 파일들이 있어야 함)\"\n",
    "        )\n",
    "\n",
    "    # config.json 수정: num_labels 같은 classification 필드 제거\n",
    "    cfg_path = p / \"config.json\"\n",
    "    try:\n",
    "        if cfg_path.exists():\n",
    "            import json\n",
    "            with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                cfg = json.load(f)\n",
    "            for k in [\"id2label\", \"label2id\", \"num_labels\", \"problem_type\"]:\n",
    "                cfg.pop(k, None)\n",
    "            with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(cfg, f, ensure_ascii=False, indent=2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    cfg = AutoConfig.from_pretrained(local_dir, local_files_only=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_dir, use_fast=True, local_files_only=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(local_dir, config=cfg, local_files_only=True)\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c754797c-3ad3-4015-b10e-fa703d16208f",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">모델 주소입력. MODEL_DIR의 경로를 꼭 모델을 저장한 경로에 맞춰서 바꿔주세요!!!!!!!</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6873e2f-608b-4d23-997e-ce2583c52821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_DIR = r\"C:\\Users\\sukoo\\Documents\\Kobart_finetuned\\kobart_finetuned\"\n",
    "GLOBAL_TOKENIZER, GLOBAL_MODEL = load_tokenizer_model(MODEL_DIR)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "GLOBAL_MODEL.to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a7958f-3690-448b-8290-7c2e6aaf971a",
   "metadata": {},
   "source": [
    "<h2>요약 호출 함수 정의</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1328ef69-96c7-4598-a6a3-4289e4630d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(tokenizer, model, device, text: str, min_new_tokens: int, max_new_tokens: int) -> str:\n",
    "    if not text or str(text).strip() == \"\":\n",
    "        return \"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            num_beams=5,\n",
    "            min_new_tokens=min_new_tokens,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            no_repeat_ngram_size=3,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "    decoded = tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
    "    cleaned = tidy_korean_summary(decoded)\n",
    "    cleaned = drop_incomplete_sentences(cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb056d94-8d9b-43df-b00f-0d423071a30c",
   "metadata": {},
   "source": [
    "<h2>후처리 함수 정의</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4c033d0b-a66a-4d8d-a8c3-b55222fa0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_korean_summary(text: str) -> str:\n",
    "    t = re.sub(r\"\\s+\", \" \", (text or \"\")).strip()\n",
    "    if t and t[-1] not in \".!?…。\\\"'”’\":\n",
    "        t += \".\"\n",
    "    return t\n",
    "\n",
    "def drop_incomplete_sentences(text: str) -> str:\n",
    "    if not text or str(text).strip() == \"\":\n",
    "        return \"\"\n",
    "    sentences = re.split(r'(?<=[\\.\\?\\!。\\!?])\\s*', text)\n",
    "    sentences = [s.strip() for s in sentences if s and s.strip()]\n",
    "    if len(sentences) == 0:\n",
    "        return \"\"\n",
    "    if len(sentences) == 1:\n",
    "        last = sentences[0]\n",
    "        if len(last.split()) < 3 or re.search(r'(며$|고$|하며$|으로$|에게$|서$|로$|에$|에 대해$|한다$|된다$)', last):\n",
    "            return \"\"\n",
    "        return last\n",
    "    last = sentences[-1]\n",
    "    if (len(last.split()) < 4) or (not re.search(r'[\\.\\?\\!。\\!?:;]$', last)) or re.search(r'(며$|고$|하며$|으로$|에게$|서$|로$|에$|에 대해$|한다$|된다$|것이다$)', last):\n",
    "        sentences = sentences[:-1]\n",
    "    cleaned = \" \".join(sentences).strip()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99ab0be-cf9c-4a77-9072-404289a67538",
   "metadata": {},
   "source": [
    "<h2>CVS 자동 읽기 함수 정의</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e5758d7c-60c3-4ee3-9fae-aef224818694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_table_auto(path: str, encoding: str = \"utf-8\"):\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".xlsx\", \".xls\"]:\n",
    "        return pd.read_excel(path, engine=\"openpyxl\")\n",
    "    elif ext in [\".tsv\", \".tab\"]:\n",
    "        return pd.read_csv(path, sep=\"\\t\", encoding=encoding)\n",
    "    else:\n",
    "        return pd.read_csv(path, encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0839590-dc11-4cf5-9877-1fe60dd98111",
   "metadata": {},
   "source": [
    "<h2>csv 요약함수 정의</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cbe09308-c1e8-4983-b3fd-6a2728f601f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_csv(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    text_col: str = \"newsContent\",\n",
    "    summary_col: str = \"summary\",\n",
    "    min_tokens: int = 40,\n",
    "    max_tokens: int = 150,\n",
    "    encoding: str = \"utf-8\",\n",
    "    device: str | None = None,\n",
    "):\n",
    "    df = _read_table_auto(input_path, encoding=encoding)\n",
    "\n",
    "    print(f\"[DEVICE] Using device: {DEVICE}\")\n",
    "\n",
    "    results = []\n",
    "    for text in tqdm(df[text_col].fillna(\"\").astype(str), desc=\"Summarizing\", ncols=120):\n",
    "        results.append(\n",
    "            summarize_text(GLOBAL_TOKENIZER, GLOBAL_MODEL, DEVICE, text, min_tokens, max_tokens)\n",
    "        )\n",
    "\n",
    "\n",
    "    df_out = df.copy()\n",
    "    df_out[summary_col] = results\n",
    "    df_out.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"[DONE] 요약 완료 → {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48eddf5-9ecc-48fe-b094-6f0a9f7275f0",
   "metadata": {},
   "source": [
    "<h2>단일 문자열 요약 함수 정의</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a4cfea33-5ba5-493c-9f39-488f7cb5db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_article(title: str, text: str) -> dict:\n",
    "    \"\"\"\n",
    "    기사 제목(title)과 본문(text)을 입력받아\n",
    "    text만 요약하고 {title, summary} 형태로 반환합니다.\n",
    "    \"\"\"\n",
    "    summary = summarize_text(\n",
    "        GLOBAL_TOKENIZER,\n",
    "        GLOBAL_MODEL,\n",
    "        DEVICE,\n",
    "        text,\n",
    "        min_new_tokens=40,\n",
    "        max_new_tokens=150\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"summary\": summary\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc48bd66-8b9c-4fa2-88c3-da8719b8be0f",
   "metadata": {},
   "source": [
    "<h2>csv파일 요약 테스트. 요약하고 싶은 csv경로를 input_path에 지정하고, 결과물을 저장할 경로를 output_path에 넣으면 됩니다.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0f902c1c-baf0-43bd-ba9a-ec74c875530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEVICE] Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing: 100%|████████████████████████████████████████████████████████████████████████| 4/4 [00:12<00:00,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] 요약 완료 → C:\\Users\\sukoo\\Documents\\test_result.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_csv(\n",
    "    input_path=r\"C:\\Users\\sukoo\\Documents\\test.csv\",\n",
    "    output_path=r\"C:\\Users\\sukoo\\Documents\\test_result.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b8cb38-aa64-4f7f-820c-6a0c5b9fe561",
   "metadata": {},
   "source": [
    "<h2>단일요약문 테스트. 기사의 title과 text를 summarize_article(title,text)로 넣어주시면 title과 summary가 딕셔너리로 저장됩니다!</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "23d13867-74c6-4189-88da-2333ab3265cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  미국이 팩트시트 담으려 한 ‘중국 견제’ 문구…곳곳에 치열한 밀당 흔적 summary:  도널드 트럼프 미국 행정부는 ‘중국 견제’라는 전략적 목표를 세우면서도 미국의 부담은 줄이고 동맹들이 비용과 역할을 늘리는 ‘동맹 현대화’를 압박해 왔는데, 이번 한미 팩트시트에도 그 내용이 곳곳에 담겼다.\n"
     ]
    }
   ],
   "source": [
    "title=\"\"\"미국이 팩트시트 담으려 한 ‘중국 견제’ 문구…곳곳에 치열한 밀당 흔적\"\"\"\n",
    "\n",
    "text=\"\"\"4일 발표된 한미 정상회담 결과를 담은 공동 팩트시트에는 ‘중국’이란 단어는 한번도 등장하지 않았다. 하지만 미국의 중국 견제와 관련한 한·미 양국의 치열한 협상 결과가 반영됐다는 게 중론이다. 도널드 트럼프 미국 행정부는 ‘중국 견제’라는 전략적 목표를 세우면서도 미국의 부담은 줄이고 동맹들이 비용과 역할을 늘리는 ‘동맹 현대화’를 압박해 왔는데, 이번 한미 팩트시트에도 그 내용이 곳곳에 담겼다.\n",
    "\n",
    "‘전략적 유연성’ 확대 요구, 적정선에서 방어\n",
    "\n",
    "먼저 눈에 띄는 대목은 ‘역내 위협에 대한 재래식 억제 태세의 강화’다. “양국은 북한을 포함해 동맹에 대한 모든 역내의 위협에 대한 미국의 재래식 억제 태세를 강화할 것”이라는 표현에 담긴 ‘역내의 위협’은 사실상 중국을 염두에 둔 것으로 볼 수 있다. 한국은 이를 위해 “미국의 지원 아래 대북 연합 재래식 방위를 주도”하며, “필수적인 군사적 역량 강화 노력을 가속”하기로 했다. 이를 위해 한국은 △국내총생산(GDP)의 3.5%까지 국방비 증액 △2030년까지 미국산 군사장비 구매에 250억 달러를 지출 △주한미군에 대한 330억달러 규모의 종합적 지원 방침을 공유했다. 주한미군의 지속 주둔과 확장억제를 유지하는 대신 한국이 미국의 인도·태평양 전략에서 역할과 비용을 훨씬 더 많이 분담하기로 하는 ‘거래형 동맹’의 현실이다.\n",
    "\n",
    "반면 ‘동맹 현대화’와 관련해 한미 양측이 “2006년 이래의 관련 양해를 확인한다”고 명시한 것은 주한미군의 ‘전략적 유연성’을 대폭 확대하려는 미국의 요구를 한국이 적절한 선에서 방어한 것으로 평가된다. 2006년 발표된 한미 공동성명에는 “전략적 유연성의 이행에 있어서, 미국은 한국이 한국민의 의지와 관계없이 동북아 지역분쟁에 개입되는 일은 없을 것이라는 한국의 입장을 존중한다”는 내용이 담겼다. 미국은 애초 이 조항을 삭제하고 주한미군이 제한 없이 대만 사태 등에 개입할 수 있는 길을 열려고 했지만, ‘2006년 수준’을 유지하려는 한국의 의도가 관철됐다.\n",
    "\n",
    "반면 ‘한·미·일 3국 협력’이 재차 강조되고, 대만해협과 남중국해, 신장위구르 등 중국이 핵심이익으로 주장하는 지역과 관련된 내용이 담긴 것은 미국의 중국 견제 전략과 관련해 주목할 부분이다.\n",
    "\n",
    "우선 팩트시트에는 ‘한·미·일 3자 협력을 강화하기로 했다’는 내용이 명시됐다. 이와 더불어 중국이 가장 민감하게 반응하는 대만 문제와 관련해서는 “대만해협에서의 평화와 안정 유지의 중요성”을 강조하고, “일방적 현상 변경에 반대”한다고 명시했다. 2023년 8월 한·미·일 캠프데이비드 선언에 담긴 ‘대만해협에서 힘에 의한 현상변경에 반대’한다는 내용과 거의 동일하다.\"\"\"\n",
    "\n",
    "result=summarize_article(title,text)\n",
    "\n",
    "title=result[\"title\"]\n",
    "\n",
    "summary=result[\"summary\"]\n",
    "\n",
    "print(\"title: \",title,\"summary: \",summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49534cf-6a63-49c3-8bd0-aded4f0ae295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
