{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8d9108",
   "metadata": {},
   "source": [
    "## ì›Œí¬í”Œë¡œìš° ìš”ì•½\n",
    "\n",
    "[ì„ í–‰ì‘ì—…] KoBARTë¡œ ê¸°ì‚¬ ë³¸ë¬¸ì˜ [ìƒì„± ìš”ì•½ë¬¸]ì„ ë¯¸ë¦¬ ì¤€ë¹„í•©ë‹ˆë‹¤.  \n",
    "[ë°ì´í„° êµ¬ì¶•] [ì •ìƒ ì œëª©]+[ìƒì„± ìš”ì•½ë¬¸] / [ë‚šì‹œì„± ì œëª©] + [ìƒì„± ìš”ì•½ë¬¸]ì„ í˜ì–´ë¡œ ë¬¶ì–´ ëŒ€ì¡° í•™ìŠµìš© ë°ì´í„°ì…‹ì„ ë§Œë“­ë‹ˆë‹¤.  \n",
    "[ëª¨ë¸ í•™ìŠµ] KoSimCSE ëª¨ë¸ì„ ContrastiveLossë¡œ íŒŒì¸íŠœë‹í•©ë‹ˆë‹¤.  \n",
    "[í‰ê°€] í•™ìŠµëœ ëª¨ë¸ë¡œ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³  ì„ê³„ê°’(Threshold)ì„ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b90679",
   "metadata": {},
   "source": [
    "## 1ë‹¨ê³„: ì„ í–‰ ì‘ì—… - KoBART ìš”ì•½ ëª¨ë¸ (ê°€ì •)\n",
    "\n",
    "ë³¸ ë…¸íŠ¸ë¶ì€ 1ë‹¨ê³„ê°€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "(ë³´ìœ í•œ <ìš”ì•½ë¬¸ ìƒì„± ë°ì´í„°>(B)ë¡œ KoBART ëª¨ë¸ì´ ì´ë¯¸ íŒŒì¸íŠœë‹ë˜ì—ˆìŠµë‹ˆë‹¤.)\n",
    "\n",
    "ì´ KoBART ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬, <ë‚šì‹œì„± ë‰´ìŠ¤ ë°ì´í„°>(A)ì˜ ëª¨ë“  ë³¸ë¬¸ì— ëŒ€í•œ [ìƒì„±ëœ ìš”ì•½ë¬¸]ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476acf8c",
   "metadata": {},
   "source": [
    "## 2ë‹¨ê³„: KoSimCSE í•™ìŠµìš© ë°ì´í„°ì…‹ êµ¬ì¶•\n",
    "KoSimCSE ëª¨ë¸ì„ 'ì§€ë„ í•™ìŠµ(Supervised)' ë°©ì‹ì˜ ëŒ€ì¡° í•™ìŠµìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "Positive Pair (ì¼ì¹˜, ë ˆì´ë¸” 1.0)\n",
    "ì…ë ¥ 1 (text1): <ë‚šì‹œì„± ë‰´ìŠ¤ ë°ì´í„°>(A.i)ì˜ [ì •ìƒ ê¸°ì‚¬ ì œëª©]\n",
    "ì…ë ¥ 2 (text2): 1ë‹¨ê³„ì—ì„œ ìƒì„±í•œ í•´ë‹¹ ê¸°ì‚¬ì˜ [ìƒì„±ëœ ìš”ì•½ë¬¸]\n",
    "\n",
    "Negative Pair (ë¶ˆì¼ì¹˜, ë ˆì´ë¸” 0.0)\n",
    "ì…ë ¥ 1 (text1): <ë‚šì‹œì„± ë‰´ìŠ¤ ë°ì´í„°>(A.ii)ì˜ [ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ê¸°ì‚¬ ì œëª©]\n",
    "ì…ë ¥ 2 (text2): 1ë‹¨ê³„ì—ì„œ ìƒì„±í•œ í•´ë‹¹ ê¸°ì‚¬ì˜ [ìƒì„±ëœ ìš”ì•½ë¬¸]\n",
    "\n",
    "**í˜„ì¬ ì¤‘ê°„ì œì¶œ ì½”ë“œì—ì„œëŠ” ìš”ì•½ë¬¸ ëª¨ë¸ì´ ì¤€ë¹„ ë˜ì§€ ì•Šì•„ 'ìš”ì•½ë¬¸' ë¶€ë¶„ì„ ë°ì´í„°ì˜ ë³¸ë¬¸ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ í…ŒìŠ¤íŠ¸í•˜ê³    \n",
    "ê·¸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ì„ ì œì‘í•˜ì˜€ìŠµë‹ˆë‹¤**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9273d1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      "Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ.\n",
      "ë°ì´í„°ë¡œ ì‚¬ìš©í•  CSV íŒŒì¼ ê²½ë¡œ (ì˜ˆìƒ): /content/gdrive/My Drive/Colab_Notebooks/ssu_NLP/new_summarization30000.csv\n",
      "CSV íŒŒì¼ì´ í•´ë‹¹ ê²½ë¡œì— ì¡´ì¬í•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#-- ë°ì´í„° ì¤€ë¹„, ì²˜ìŒ í•œë²ˆë§Œ í•˜ë©´ ë˜ë‹ˆê°„ ì „ì²´ ì£¼ì„ì²˜ë¦¬!\n",
    "\n",
    "# 1. Google Drive ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "print(\"Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ.\")\n",
    "\n",
    "# 2. (NEW) CSV íŒŒì¼ ê²½ë¡œ í™•ì¸ (ì˜ˆì‹œ)\n",
    "# ì‹¤ì œ ë°ì´í„° ë¡œë“œëŠ” 8ë²ˆ ì…€(f3cee925)ì—ì„œ ì´ë¤„ì§‘ë‹ˆë‹¤.\n",
    "import os\n",
    "\n",
    "# â˜…â˜…â˜… ì‚¬ìš©ìë‹˜ì´ ë§ì”€í•˜ì‹  CSV íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œë¥¼ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”. â˜…â˜…â˜…\n",
    "csv_path = \"/content/gdrive/My Drive/Colab_Notebooks/ssu_NLP/new_summarization30000.csv\"\n",
    "print(f\"ë°ì´í„°ë¡œ ì‚¬ìš©í•  CSV íŒŒì¼ ê²½ë¡œ (ì˜ˆìƒ): {csv_path}\")\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    print(\"--- ğŸ”´ ê²½ê³  ---\")\n",
    "    print(\"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 8ë²ˆ ì…€ì˜ 'csv_path'ë¥¼ ì •í™•íˆ ìˆ˜ì •í•´ì£¼ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"CSV íŒŒì¼ì´ í•´ë‹¹ ê²½ë¡œì— ì¡´ì¬í•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# --- ê¸°ì¡´ ì••ì¶• í•´ì œ ë¡œì§ì€ ì£¼ì„ ì²˜ë¦¬ ---\n",
    "# print(\"ê¸°ì¡´ ZIP ì••ì¶• í•´ì œ ë¡œì§ì€ new_summarization30000.csv íŒŒì¼ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8033384",
   "metadata": {},
   "source": [
    "## 3ë‹¨ê³„: KoSimCSE ëª¨ë¸ êµ¬ì„± ë° íŒŒì¸íŠœë‹\n",
    "sentence-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ êµ¬ì„±í•˜ê³  í•™ìŠµì‹œí‚µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fcddf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n%pip install sentence-transformers torch\\n\\n#ê¸°íƒ€ í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬\\n%pip install datasets\\n%pip install accelerate\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-- 3.1. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (Jupyter/Colab í™˜ê²½)\n",
    "'''\n",
    "%pip install sentence-transformers torch\n",
    "\n",
    "#ê¸°íƒ€ í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "%pip install datasets\n",
    "%pip install accelerate\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608cf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# 3.2. ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, util\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b674b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV íŒŒì¼ ë¡œë“œ ì‹œë„: /content/gdrive/My Drive/Colab_Notebooks/ssu_NLP/new_summarization30000.csv\n",
      "CSV ë¡œë“œ ì™„ë£Œ. ì´ 30003ê°œì˜ í–‰ ë°œê²¬.\n",
      "ê²°ì¸¡ì¹˜ ì œê±° í›„ ìœ íš¨í•œ í–‰: 30003ê°œ\n",
      "\n",
      "--- ë°ì´í„° ë¡œë“œ ë° ë¶„ë¦¬ ì™„ë£Œ ---\n",
      "  ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬í•œ CSV í–‰ ìˆ˜: 30003 ê°œ\n",
      "  ìƒì„±ëœ ì´ í•™ìŠµ í˜ì–´(ìŒ): 30003 ê°œ\n",
      "  - (í•™ìŠµìš©)  train_data_list: 27002 ê°œ (90%)\n",
      "  - (ê²€ì¦ìš©) validation_data_list: 3001 ê°œ (10%)\n",
      "\n",
      "ì´ 27002ê°œì˜ í•™ìŠµ ì˜ˆì‹œ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# 3.3. í•™ìŠµ ë°ì´í„° ì •ì˜ (â˜…CSV íŒŒì¼ì—ì„œ ë¡œë“œ ë° ë¶„ë¦¬â˜…)\n",
    "# new_summarization30000.csv íŒŒì¼ì—ì„œ (text1, text2, label) í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "# 1. í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "import random\n",
    "import os\n",
    "import pandas as pd # (NEW) Pandas ì„í¬íŠ¸\n",
    "\n",
    "# 2. (NEW) ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •\n",
    "# â˜…â˜…â˜… ì´ ê²½ë¡œëŠ” 4ë²ˆ ì…€ì—ì„œ í™•ì¸í•œ ê²½ë¡œì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤. â˜…â˜…â˜…\n",
    "csv_path = \"/content/gdrive/My Drive/Colab_Notebooks/ssu_NLP/new_summarization30000.csv\"\n",
    "\n",
    "# 3. (NEW) all_data_pairs ìƒì„± (Pandas ë¡œì§)\n",
    "all_data_pairs = []\n",
    "total_rows_processed = 0\n",
    "\n",
    "print(f\"CSV íŒŒì¼ ë¡œë“œ ì‹œë„: {csv_path}\")\n",
    "\n",
    "try:\n",
    "    # 3.1. CSV íŒŒì¼ ë¡œë“œ\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"CSV ë¡œë“œ ì™„ë£Œ. ì´ {len(df)}ê°œì˜ í–‰ ë°œê²¬.\")\n",
    "\n",
    "    # (NEW) 3.2. ì‚¬ìš©í•  ì»¬ëŸ¼(newsTitle, summary, clickbaitClass)ì— ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ì œê±°\n",
    "    df = df.dropna(subset=['newsTitle', 'summary', 'clickbaitClass'])\n",
    "    print(f\"ê²°ì¸¡ì¹˜ ì œê±° í›„ ìœ íš¨í•œ í–‰: {len(df)}ê°œ\")\n",
    "\n",
    "    # 3.3. ë°ì´í„°í”„ë ˆì„ ìˆœíšŒ\n",
    "    # (ì´ë¯¸ì§€ì—ì„œ í™•ì¸í•œ ì»¬ëŸ¼ëª…: 'newsTitle', 'summary', 'clickbaitClass' ì‚¬ìš©)\n",
    "    for row in df.to_dict('records'):\n",
    "\n",
    "        title = row.get('newsTitle')\n",
    "        summary_text = row.get('summary')\n",
    "        class_label = row.get('clickbaitClass') # (0 ë˜ëŠ” 1)\n",
    "\n",
    "        # (PandasëŠ” ê²°ì¸¡ì¹˜ë¥¼ NaN (float)ìœ¼ë¡œ ë‹¤ë£¸)\n",
    "        def is_str_valid(s):\n",
    "            return s and isinstance(s, str) and s.strip()\n",
    "\n",
    "        # ì œëª©ê³¼ ìš”ì•½ë¬¸ì´ ìœ íš¨í•œ ë¬¸ìì—´ì¸ì§€ í™•ì¸\n",
    "        if is_str_valid(title) and is_str_valid(summary_text):\n",
    "            try:\n",
    "                # class_labelì´ 0, 1, \"0\", \"1\", 0.0 ë“± ì–´ë–¤ í˜•íƒœë“  intë¡œ ë³€í™˜\n",
    "                class_val_int = int(float(class_label))\n",
    "\n",
    "                if class_val_int == 0:\n",
    "                    # 0 = ì •ìƒ ê¸°ì‚¬ -> Positive Pair (Label 1.0)\n",
    "                    final_label = 1.0\n",
    "                elif class_val_int == 1:\n",
    "                    # 1 = ë‚šì‹œì„± ê¸°ì‚¬ -> Negative Pair (Label 0.0)\n",
    "                    final_label = 0.0\n",
    "                else:\n",
    "                    continue # 0ì´ë‚˜ 1ì´ ì•„ë‹ˆë©´ ê±´ë„ˆëœ€\n",
    "\n",
    "                # (ì œëª©, ìš”ì•½ë¬¸, ë‚šì‹œì„±_ì—¬ë¶€_ë ˆì´ë¸”)\n",
    "                all_data_pairs.append((title, summary_text, final_label))\n",
    "                total_rows_processed += 1\n",
    "\n",
    "            except (ValueError, TypeError):\n",
    "                # int ë³€í™˜ ì‹¤íŒ¨ ì‹œ (ì˜ˆ: class_labelì´ ì´ìƒí•œ ê°’ì¼ ê²½ìš°)\n",
    "                continue\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"--- ğŸ”´ ì˜¤ë¥˜: {csv_path} ì—ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ---\")\n",
    "    print(\"--- 4ë²ˆ ì…€(drive.mount)ì´ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€, íŒŒì¼ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”. ---\")\n",
    "    df = pd.DataFrame() # ë¹ˆ ë°ì´í„°í”„ë ˆì„\n",
    "except KeyError as e:\n",
    "    print(f\"--- ğŸ”´ ì˜¤ë¥˜: CSVì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤ ---\")\n",
    "    print(f\"'{e.args[0]}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (newsTitle, summary, clickbaitClass ì¤‘ í•˜ë‚˜)\")\n",
    "except Exception as e:\n",
    "    print(f\"CSV ë¡œë“œ ë˜ëŠ” ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "\n",
    "# (ê¸°ì¡´ ë¡œì§) 4. ë°ì´í„° ì…”í”Œ ë° ë¶„ë¦¬\n",
    "if all_data_pairs:\n",
    "    random.shuffle(all_data_pairs) # â˜…ì „ì²´ ë°ì´í„°ë¥¼ 1íšŒ ì…”í”Œ\n",
    "\n",
    "    # (NEW) 90%ëŠ” í•™ìŠµ, 10%ëŠ” ê²€ì¦(Validation)ìš©ìœ¼ë¡œ ë¶„ë¦¬\n",
    "    validation_split_ratio = 0.1\n",
    "    split_index = int(len(all_data_pairs) * (1 - validation_split_ratio))\n",
    "\n",
    "    train_data_list = all_data_pairs[:split_index]\n",
    "    validation_data_list = all_data_pairs[split_index:] # 12ë²ˆ ì…€ì—ì„œ ì‚¬ìš©í•  ê²€ì¦ ë°ì´í„°\n",
    "\n",
    "    print(f\"\\n--- ë°ì´í„° ë¡œë“œ ë° ë¶„ë¦¬ ì™„ë£Œ ---\")\n",
    "    print(f\"  ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬í•œ CSV í–‰ ìˆ˜: {total_rows_processed} ê°œ\")\n",
    "    print(f\"  ìƒì„±ëœ ì´ í•™ìŠµ í˜ì–´(ìŒ): {len(all_data_pairs)} ê°œ\")\n",
    "    print(f\"  - (í•™ìŠµìš©)  train_data_list: {len(train_data_list)} ê°œ (90%)\")\n",
    "    print(f\"  - (ê²€ì¦ìš©) validation_data_list: {len(validation_data_list)} ê°œ (10%)\")\n",
    "\n",
    "else:\n",
    "    print(\"ê²½ê³ : í•™ìŠµ ë°ì´í„°ë¥¼ í•˜ë‚˜ë„ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œì™€ CSV ì»¬ëŸ¼ëª…ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    train_data_list = []\n",
    "    validation_data_list = []\n",
    "\n",
    "print(f\"\\nì´ {len(train_data_list)}ê°œì˜ í•™ìŠµ ì˜ˆì‹œ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee1a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac0940ced264fce91f5a3498e2e2881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/764 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49794cf0a8c840a0829bf6c153a5722f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56d104416da471ead19ade7fed03e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bebb4156fb24cda8c5a0f1e3690a6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5314c800d824438c963da89c894501c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ecb872e86441cd8d637771d885d28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ 'BM-K/KoSimCSE-roberta-multitask' ë¡œë“œ ë° '[CLS] í’€ë§' ì ìš© ì™„ë£Œ.\n",
      "ì‚¬ìš© ì¥ì¹˜: cuda\n"
     ]
    }
   ],
   "source": [
    "# 3.4.0. í•„ìš”í•œ ëª¨ë“ˆ ì¶”ê°€ ì„í¬íŠ¸\n",
    "from sentence_transformers import models\n",
    "\n",
    "# 3.4.1. ì‚¬ìš©í•  KoSimCSE ëª¨ë¸ ì´ë¦„ (ì‚¬ìš©ìë‹˜ì´ ì°¾ìœ¼ì‹  ëª¨ë¸)\n",
    "model_name = 'BM-K/KoSimCSE-roberta-multitask'\n",
    "\n",
    "# 3.4.2. CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ì²´í¬\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 3.4.3. SentenceTransformer ëª¨ë¸ ìˆ˜ë™ ì¡°ë¦½\n",
    "try:\n",
    "    # 1. 'ì—”ì§„' (ê¸°ë³¸ Transformer ëª¨ë¸) ë¡œë“œ\n",
    "    word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "    # 2. 'í’€ë§ ë ˆì´ì–´' ì •ì˜\n",
    "    # SimCSEëŠ” ë³´í†µ [CLS] í† í°ì˜ ì„ë² ë”©ì„ ë¬¸ì¥ ë²¡í„°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    pooling_model = models.Pooling(\n",
    "        word_embedding_model.get_word_embedding_dimension(),\n",
    "        pooling_mode='cls' # 'mean'ì´ ì•„ë‹Œ 'cls'ë¡œ ëª…ì‹œì  ì§€ì •\n",
    "    )\n",
    "\n",
    "    # 3. ë‘ ëª¨ë“ˆì„ ê²°í•©í•˜ì—¬ ìµœì¢… SentenceTransformer ëª¨ë¸ ìƒì„±\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model], device=device)\n",
    "\n",
    "    print(f\"ëª¨ë¸ '{model_name}' ë¡œë“œ ë° '[CLS] í’€ë§' ì ìš© ì™„ë£Œ.\")\n",
    "    print(f\"ì‚¬ìš© ì¥ì¹˜: {device}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print(\"Hugging Face Hubì— ì—°ê²°í•  ìˆ˜ ìˆëŠ”ì§€, ëª¨ë¸ ì´ë¦„ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015795d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 27002ê°œì˜ InputExample ìƒì„± ì™„ë£Œ\n",
      "\n",
      "--- ì²« ë²ˆì§¸ í•™ìŠµ ì˜ˆì‹œ --- \n",
      "Text 1: ë¬´ì„­ê²Œ ì¹˜ì†ŸëŠ” ç¾ ì„ëŒ€ë£Œâ€¦ë§¨í•´íŠ¼ ì•„íŒŒíŠ¸ ì›”ì„¸ ì¤‘ìœ„ê°’ '502ë§Œì›'\n",
      "Text 2: ë¯¸êµ­ì˜ ì£¼íƒ ì„ëŒ€ë£Œê°€ ê°€íŒŒë¥´ê²Œ ìƒìŠ¹í•˜ëŠ” ê°€ìš´ë° ë§¨í•´íŠ¼ ì§€ì—­ ì•„íŒŒíŠ¸ ì›”ì„¸ ì¤‘ìœ„ê°€ê²©ì´ ì²˜ìŒìœ¼ë¡œ 4000ë‹¬ëŸ¬(ì•½ 502ë§Œì›)ì„ ëŒíŒŒí–ˆë‹¤ê³  ë¸”ë£¸ë²„ê·¸ í†µì‹ ì´ 8ì¼(í˜„ì§€ì‹œê°„) ë°í˜”ëŠ”ë°, ì´ ì¤‘ìœ„ê°’ì€ ì•„íŒŒíŠ¸ ë§¤ë§¤ê°€ê²©ì„ ì¼ë ¬ë¡œ ì„¸ì› ì„ ë•Œ ì •ì¤‘ì•™ì— ìœ„ì¹˜í•˜ëŠ” ê°€ê²©ì„ ì˜ë¯¸í•œë‹¤.\n",
      "Label: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 3.5. í•™ìŠµ ë°ì´í„°ì…‹ í˜•ì‹ ë³€í™˜ (InputExample)\n",
    "# sentence-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ContrastiveLossëŠ” InputExample í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# ì—¬ê¸°ì„œ texts ë¦¬ìŠ¤íŠ¸ì—ëŠ” [text1, text2]ê°€, labelì—ëŠ” 0.0 ë˜ëŠ” 1.0ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤.\n",
    "\n",
    "# (ì´ì „ 3.3. ì…€ì—ì„œ train_data_listê°€ ì •ì˜ë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.)\n",
    "\n",
    "train_examples = []\n",
    "for data in train_data_list:\n",
    "    text1 = data[0]\n",
    "    text2 = data[1]\n",
    "    label = float(data[2])\n",
    "    train_examples.append(InputExample(texts=[text1, text2], label=label))\n",
    "\n",
    "print(f\"ì´ {len(train_examples)}ê°œì˜ InputExample ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# ì²« ë²ˆì§¸ ì˜ˆì‹œ í™•ì¸\n",
    "if train_examples:\n",
    "    print(\"\\n--- ì²« ë²ˆì§¸ í•™ìŠµ ì˜ˆì‹œ --- \")\n",
    "    print(f\"Text 1: {train_examples[0].texts[0]}\")\n",
    "    print(f\"Text 2: {train_examples[0].texts[1]}\")\n",
    "    print(f\"Label: {train_examples[0].label}\")\n",
    "\n",
    "#--\n",
    "# Text 1: ê¸°ì‚¬ì˜ **'í—¤ë“œë¼ì¸' (ì œëª©)**ì…ë‹ˆë‹¤. --> (Labelì´ 1.0ì´ë©´ newsTitle - ì •ìƒ ì œëª©) / (Labelì´ 0.0ì´ë©´ newTitle - ë‚šì‹œì„± ì œëª©)\n",
    "# Text 2: ê¸°ì‚¬ì˜ **'ë³¸ë¬¸' (newsContent)**ì…ë‹ˆë‹¤. (í˜„ì¬ëŠ” ì›ë¬¸ ì „ì²´ì´ê³ , 7ë²ˆ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ 512ê¸€ìë¡œ ì˜ë¦° ë³¸ë¬¸ì´ ë©ë‹ˆë‹¤.)\n",
    "\n",
    "# ì¶œë ¥ëœ ì˜ˆì‹œ(Label: 0.0)ëŠ” ì´ ë‘ í…ìŠ¤íŠ¸ê°€ ì„œë¡œ **'ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ë‚šì‹œì„± ê¸°ì‚¬ ìŒ'**ì´ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c8153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader ë° ContrastiveLoss ì •ì˜ ì™„ë£Œ\n",
      "ë¯¸ë¦¬ ë¶„ë¦¬ëœ ê²€ì¦(validation) ì„¸íŠ¸ 3001ê°œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "í‰ê°€ì(evaluator) ì¤€ë¹„ ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "#-- 3.6.a í•™ìŠµ ì„¤ì • (DataLoader ë° Contrastive Loss)\n",
    "# DataLoader: í•™ìŠµ ë°ì´í„°ë¥¼ ë°°ì¹˜(batch) ë‹¨ìœ„ë¡œ ëª¨ë¸ì— ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "# ContrastiveLoss: í”„ë¡œì íŠ¸ ê³„íš 5.Bì—ì„œ ì œì•ˆí•œ ëŒ€ì¡° í•™ìŠµ ì†ì‹¤ í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "# label=1.0 (Positive) í˜ì–´ëŠ” ì„ë² ë”© ê±°ë¦¬ë¥¼ ê°€ê¹ê²Œ (0ì— ê°€ê¹ê²Œ) ë§Œë“­ë‹ˆë‹¤.\n",
    "# label=0.0 (Negative) í˜ì–´ëŠ” ì„ë² ë”© ê±°ë¦¬ë¥¼ margin ê°’ë³´ë‹¤ ë©€ê²Œ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "# DataLoader ìƒì„±\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32) # 2 -> 32ë¡œ ë³€ê²½\n",
    "\n",
    "# Loss í•¨ìˆ˜ ì •ì˜: ContrastiveLoss\n",
    "# margin: Negative Pairê°€ ê°€ì ¸ì•¼ í•  ìµœì†Œ ê±°ë¦¬ (ê¸°ë³¸ê°’ 0.5)\n",
    "train_loss = losses.ContrastiveLoss(model=model, margin=0.5)\n",
    "\n",
    "print(\"DataLoader ë° ContrastiveLoss ì •ì˜ ì™„ë£Œ\")\n",
    "\n",
    "\n",
    "\n",
    "#-- 3.6.b. ê²€ì¦(Validation) ë°ì´í„°ì…‹ ë° í‰ê°€ì(Evaluator) ì¤€ë¹„ ---\n",
    "from sentence_transformers.evaluation import BinaryClassificationEvaluator\n",
    "import random\n",
    "\n",
    "# (ì´ì „ 3.5 ì…€ì˜ train_examplesê°€ ì •ì˜ë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤)\n",
    "\n",
    "# --- [ â˜… ìˆ˜ì •ëœ ë¶€ë¶„ â˜… ] ---\n",
    "# (8ë²ˆ ì…€ì—ì„œ ë¯¸ë¦¬ ë¶„ë¦¬í•´ ë‘” 'validation_data_list'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤)\n",
    "if 'validation_data_list' in locals() and validation_data_list:\n",
    "\n",
    "    print(f\"ë¯¸ë¦¬ ë¶„ë¦¬ëœ ê²€ì¦(validation) ì„¸íŠ¸ {len(validation_data_list)}ê°œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    # í‰ê°€ìê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë¶„ë¦¬\n",
    "    val_sentences1 = [data[0] for data in validation_data_list]\n",
    "    val_sentences2 = [data[1] for data in validation_data_list]\n",
    "    val_labels = [data[2] for data in validation_data_list]\n",
    "\n",
    "    # BinaryClassificationEvaluator: (ë¬¸ì¥1, ë¬¸ì¥2, ë ˆì´ë¸”[0/1]) í˜•ì‹ì— ì‚¬ìš©\n",
    "    evaluator = BinaryClassificationEvaluator(\n",
    "        val_sentences1,\n",
    "        val_sentences2,\n",
    "        val_labels,\n",
    "        name='validation'\n",
    "    )\n",
    "    print(\"í‰ê°€ì(evaluator) ì¤€ë¹„ ì™„ë£Œ.\")\n",
    "else:\n",
    "    print(\"ê²€ì¦ ì„¸íŠ¸(validation_data_list)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. evaluator ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "    evaluator = None\n",
    "# --- [ â˜… ìˆ˜ì •ëœ ë¶€ë¶„ â˜… ] ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d9955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ìŠ¤í… ìˆ˜: 4220, Warmup ìŠ¤í… ìˆ˜: 211 (5%~)\n",
      "ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œì‘... (Epochs: 5)\n",
      "â˜…â˜…â˜… (ì¤‘ìš”) ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ Google Drive ê²½ë¡œì— ì €ì¥ë©ë‹ˆë‹¤: /content/gdrive/MyDrive/Colab_Notebooks/ssu_NLP/Trained_simcse_model/model_251114_0548 â˜…â˜…â˜…\n",
      "ê²€ì¦ì(evaluator)ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë§¤ ì—í¬í¬(Epoch) ì¢…ë£Œ ì‹œ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9458471bc44ede949b4058c0b54fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4220' max='4220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4220/4220 1:02:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation Cosine Accuracy</th>\n",
       "      <th>Validation Cosine Accuracy Threshold</th>\n",
       "      <th>Validation Cosine F1</th>\n",
       "      <th>Validation Cosine F1 Threshold</th>\n",
       "      <th>Validation Cosine Precision</th>\n",
       "      <th>Validation Cosine Recall</th>\n",
       "      <th>Validation Cosine Ap</th>\n",
       "      <th>Validation Cosine Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>844</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.793735</td>\n",
       "      <td>0.768730</td>\n",
       "      <td>0.843029</td>\n",
       "      <td>0.747361</td>\n",
       "      <td>0.824828</td>\n",
       "      <td>0.862051</td>\n",
       "      <td>0.907296</td>\n",
       "      <td>0.533809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1688</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.799067</td>\n",
       "      <td>0.770260</td>\n",
       "      <td>0.848871</td>\n",
       "      <td>0.712703</td>\n",
       "      <td>0.813735</td>\n",
       "      <td>0.887179</td>\n",
       "      <td>0.913254</td>\n",
       "      <td>0.535722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2532</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.796734</td>\n",
       "      <td>0.822166</td>\n",
       "      <td>0.847533</td>\n",
       "      <td>0.728715</td>\n",
       "      <td>0.813000</td>\n",
       "      <td>0.885128</td>\n",
       "      <td>0.916958</td>\n",
       "      <td>0.532036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3376</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.794402</td>\n",
       "      <td>0.741111</td>\n",
       "      <td>0.844087</td>\n",
       "      <td>0.726365</td>\n",
       "      <td>0.826857</td>\n",
       "      <td>0.862051</td>\n",
       "      <td>0.916356</td>\n",
       "      <td>0.537937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4220</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.789737</td>\n",
       "      <td>0.773026</td>\n",
       "      <td>0.838645</td>\n",
       "      <td>0.697621</td>\n",
       "      <td>0.821130</td>\n",
       "      <td>0.856923</td>\n",
       "      <td>0.913174</td>\n",
       "      <td>0.521335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sentence_transformers/util/tensor.py:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  a = torch.tensor(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ Google Drive '/content/gdrive/MyDrive/Colab_Notebooks/ssu_NLP/Trained_simcse_model/model_251114_0548' ê²½ë¡œì— ì €ì¥ë¨)\n",
      "\n",
      "--- ğŸš€ í•™ìŠµ ìµœì¢… ìš”ì•½ ---\n",
      "  ì´ í•™ìŠµ ì‹œê°„  : 3739.88 ì´ˆ\n",
      "  (í•™ìŠµ ì¤‘ ì¶œë ¥ëœ INFO ë¡œê·¸ë¡œ ì„±ëŠ¥ í–¥ìƒì„ í™•ì¸í•˜ì„¸ìš”)\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "#-- 3.7. ëª¨ë¸ í•™ìŠµ ì‹¤í–‰\n",
    "# ì´ì œ model.fit()ì„ í˜¸ì¶œí•˜ì—¬ íŒŒì¸íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "# (NEW) Google Driveì˜ ê³ ìœ í•œ ê²½ë¡œì— ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ ì €ì¥ë©ë‹ˆë‹¤.\n",
    "\n",
    "# (NEW) 1. ë¡œê¹… ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "import logging\n",
    "import time\n",
    "import os     # (NEW) os ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "from datetime import datetime # (NEW) ë‚ ì§œ/ì‹œê°„ ì„í¬íŠ¸\n",
    "\n",
    "# (NEW) 2. ë¡œê¹… ì„¤ì • (INFO ë ˆë²¨)\n",
    "# model.fit()ì´ INFO ë ˆë²¨ì˜ ë¡œê·¸ë¥¼ ì¶œë ¥í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "# (NEW) 3. 'wandb' ë¡œê¹… ê°•ì œ ë¹„í™œì„±í™” (API í‚¤ í”„ë¡¬í”„íŠ¸ ë°©ì§€)\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "num_epochs = 5 # ì—í¬í¬ (ì‹¤ì œ ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œëŠ” 1~3 ì‚¬ì´ë¡œ ì¡°ì ˆ)\n",
    "\n",
    "# (NEW) warmup_stepsë¥¼ ì‹¤ì œ ìŠ¤í… ìˆ˜ì— ë§ê²Œ ì¡°ì • (ì˜ˆ: ì „ì²´ ìŠ¤í…ì˜ 5%)\n",
    "# (10ë²ˆ ì…€ì—ì„œ len(train_dataloader)ê°€ ì •ì˜ë˜ì—ˆë‹¤ê³  ê°€ì •)\n",
    "try:\n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "    # warmup_stepsëŠ” ìµœì†Œ 100 ì´ìƒìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì•ˆì •ì ì…ë‹ˆë‹¤.\n",
    "    warmup_steps_count = max(100, int(total_steps * 0.05))\n",
    "    print(f\"ì´ ìŠ¤í… ìˆ˜: {total_steps}, Warmup ìŠ¤í… ìˆ˜: {warmup_steps_count} (5%~)\")\n",
    "except NameError:\n",
    "    print(\"ì˜¤ë¥˜: train_dataloaderê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 10ë²ˆ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "    warmup_steps_count = 100 # ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì„¤ì •\n",
    "    total_steps = 0\n",
    "\n",
    "# --- [ â˜…â˜…â˜… (í†µí•©) ë™ì  ì €ì¥ ê²½ë¡œ ìƒì„± â˜…â˜…â˜… ] ---\n",
    "# 1. ê¸°ë³¸ ì €ì¥ í´ë” (ì‚¬ìš©ìë‹˜ì´ ìš”ì²­í•˜ì‹  ê²½ë¡œ)\n",
    "# (ì£¼ì˜: Colabì˜ 'My Drive'ëŠ” 'MyDrive'ë¡œ ë„ì–´ì“°ê¸° ì—†ì´ ë¶™ì—¬ ì¨ì•¼ í•©ë‹ˆë‹¤)\n",
    "base_save_dir = \"/content/gdrive/MyDrive/Colab_Notebooks/ssu_NLP/Trained_simcse_model\"\n",
    "\n",
    "# 2. (NEW) í˜„ì¬ ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ _YYMMDD_HHMM í˜•ì‹ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±\n",
    "#    (ì˜ˆ: _251112_1841)\n",
    "current_time_str = datetime.now().strftime(\"_%y%m%d_%H%M\")\n",
    "\n",
    "# 3. (NEW) ìµœì¢… ëª¨ë¸ í´ë”ëª… (ì˜ˆ: model_251112_1841)\n",
    "model_folder_name = f\"model{current_time_str}\"\n",
    "\n",
    "# 4. ìµœì¢… ì €ì¥ ê²½ë¡œ\n",
    "model_save_path = os.path.join(base_save_dir, model_folder_name)\n",
    "\n",
    "# 5. (NEW) ê¸°ë³¸ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "os.makedirs(base_save_dir, exist_ok=True)\n",
    "# --- [ â˜…â˜…â˜… (í†µí•©) ìˆ˜ì • ë â˜…â˜…â˜… ] ---\n",
    "\n",
    "\n",
    "print(f\"ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œì‘... (Epochs: {num_epochs})\")\n",
    "print(f\"â˜…â˜…â˜… (ì¤‘ìš”) ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ Google Drive ê²½ë¡œì— ì €ì¥ë©ë‹ˆë‹¤: {model_save_path} â˜…â˜…â˜…\")\n",
    "\n",
    "# (ì´ì „ ì…€ì—ì„œ evaluatorê°€ ìƒì„±ë˜ì—ˆë‹¤ê³  ê°€ì •)\n",
    "if 'evaluator' in locals() and evaluator:\n",
    "    print(\"ê²€ì¦ì(evaluator)ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë§¤ ì—í¬í¬(Epoch) ì¢…ë£Œ ì‹œ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"ê²€ì¦ì(evaluator)ê°€ ì—†ì–´, ì—í¬í¬ê°€ ëë‚  ë•Œë§Œ ë¡œê¹…ë©ë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# (NEW) í•™ìŠµ ì‹œê°„ ì¸¡ì •ì„ ìœ„í•œ ì‹œì‘ ì‹œê°„ ê¸°ë¡\n",
    "start_time = time.time()\n",
    "\n",
    "# ëª¨ë¸ íŒŒì¸íŠœë‹\n",
    "# (3.4ì˜ 'model' ë³€ìˆ˜ì™€ 3.6ì˜ 'train_loss' ë³€ìˆ˜ë¥¼ ì‚¬ìš©)\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "\n",
    "          # --- [ â˜…â˜…â˜… (í†µí•©) ìˆ˜ì •ëœ ë¶€ë¶„ â˜…â˜…â˜… ] ---\n",
    "          epochs=num_epochs,               # 'ì§„ì§œ' ì—í¬í¬ ì‚¬ìš©\n",
    "          evaluator=evaluator if 'evaluator' in locals() else None, # ì´ì „ ì…€ì—ì„œ ë§Œë“  í‰ê°€ì\n",
    "          save_best_model=True,          # (NEW) â˜…Trueë¡œ ë³€ê²½â˜…: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "          # --- [ â˜…â˜…â˜… (í†µí•©) ìˆ˜ì • ë â˜…â˜…â˜… ] ---\n",
    "\n",
    "          warmup_steps=warmup_steps_count,\n",
    "          output_path=model_save_path, # ì´ ê²½ë¡œê°€ Google Driveì˜ ê³ ìœ í•œ í´ë”ë¥¼ ê°€ë¦¬í‚µë‹ˆë‹¤.\n",
    "          show_progress_bar=True) # ì§„í–‰ ë°”ëŠ” ë‹¤ì‹œ ì¼œë„ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "\n",
    "# (NEW) ì¢…ë£Œ ì‹œê°„ ê¸°ë¡ ë° ì´ ì‹œê°„ ê³„ì‚°\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# (NEW) â˜…ìµœì¢… ì¶œë ¥ ë©”ì‹œì§€ ìˆ˜ì •\n",
    "print(f\"\\nëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ Google Drive '{model_save_path}' ê²½ë¡œì— ì €ì¥ë¨)\")\n",
    "\n",
    "# (NEW) 5. í•™ìŠµ ê²°ê³¼(ì‹œê°„)ë¥¼ ê°€ì‹œì„± ì¢‹ê²Œ ì¶œë ¥\n",
    "print(\"\\n--- ğŸš€ í•™ìŠµ ìµœì¢… ìš”ì•½ ---\")\n",
    "print(f\"  ì´ í•™ìŠµ ì‹œê°„  : {total_time:.2f} ì´ˆ\")\n",
    "print(\"  (í•™ìŠµ ì¤‘ ì¶œë ¥ëœ INFO ë¡œê·¸ë¡œ ì„±ëŠ¥ í–¥ìƒì„ í™•ì¸í•˜ì„¸ìš”)\")\n",
    "print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc09b75e",
   "metadata": {},
   "source": [
    "## 4ë‹¨ê³„: ëª¨ë¸ í‰ê°€ ë° ì„ê³„ê°’(Threshold) ì„¤ì •\n",
    "í•™ìŠµëœ ëª¨ë¸(í˜¹ì€ ì €ì¥ëœ ëª¨ë¸)ì„ ì‚¬ìš©í•˜ì—¬ 'ì •ìƒ'ê³¼ 'ë‚šì‹œì„±'ì„ ì–¼ë§ˆë‚˜ ì˜ êµ¬ë¶„í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c3d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7 ì…€ì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤: /content/gdrive/MyDrive/Colab_Notebooks/ssu_NLP/Trained_simcse_model/model_251114_0548\n",
      "ì €ì¥ëœ ëª¨ë¸ '/content/gdrive/MyDrive/Colab_Notebooks/ssu_NLP/Trained_simcse_model/model_251114_0548' ë¡œë“œ ì™„ë£Œ\n",
      "í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì„ë² ë”© ì¤‘...\n",
      "\n",
      "--- ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼ ---\n",
      "[í…ŒìŠ¤íŠ¸ 1: ì •ìƒ (ì˜ë¯¸ ì¼ì¹˜)] (ê¸°ëŒ€ê°’: ë§¤ìš° ë†’ìŒ) \t ìœ ì‚¬ë„: 0.9604\n",
      "[í…ŒìŠ¤íŠ¸ 2: ë‚šì‹œì„± (ì˜ë¯¸ ë°˜ëŒ€)] (ê¸°ëŒ€ê°’: ë‚®ìŒ) \t ìœ ì‚¬ë„: 0.9459\n",
      "[í…ŒìŠ¤íŠ¸ 3: ë‚šì‹œì„± (ì• ë§¤í•¨)] (ê¸°ëŒ€ê°’: ì¤‘ê°„/ë‚®ìŒ) \t ìœ ì‚¬ë„: 0.9402\n",
      "[í…ŒìŠ¤íŠ¸ 4: ë¬´ê´€ (ì£¼ì œ ë‹¤ë¦„)] (ê¸°ëŒ€ê°’: ë§¤ìš° ë‚®ìŒ) \t ìœ ì‚¬ë„: 0.8916\n"
     ]
    }
   ],
   "source": [
    "#-- 4.1. í•™ìŠµëœ ëª¨ë¸ë¡œ ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸ (â˜… 3.7ì…€ ì‹¤í–‰ ì§í›„ ì‚¬ìš© â˜…)\n",
    "\n",
    "# --- [ â˜…â˜…â˜… ì¤‘ìš” â˜…â˜…â˜… ] ---\n",
    "# ì´ ì…€ì€ 3.7 ì…€ ì‹¤í–‰ 'ì§í›„'ì— ëŸ°íƒ€ì„ì´ ì¬ì‹œì‘ë˜ì§€ ì•Šì•˜ì„ ë•Œë§Œ ì‘ë™í•©ë‹ˆë‹¤.\n",
    "# 3.7 ì…€ì—ì„œ ìƒì„±ëœ 'model_save_path' ë³€ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "try:\n",
    "    # (model_save_pathëŠ” 3.7 ì…€ì—ì„œ ì •ì˜ë¨)\n",
    "    print(f\"3.7 ì…€ì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤: {model_save_path}\")\n",
    "    model = SentenceTransformer(model_save_path)\n",
    "    print(f\"ì €ì¥ëœ ëª¨ë¸ '{model_save_path}' ë¡œë“œ ì™„ë£Œ\")\n",
    "except NameError:\n",
    "     print(f\"--- ğŸ”´ ì˜¤ë¥˜: 'model_save_path' ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ---\")\n",
    "     print(\"ëŸ°íƒ€ì„ì´ ì¬ì‹œì‘ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë…¸íŠ¸ë¶ ìƒë‹¨ì˜ [ì…€ 4.1 (B) ìˆ˜ë™ ë¡œë“œ] ì…€ì„ ëŒ€ì‹  ì‚¬ìš©í•˜ì„¸ìš”.\")\n",
    "except Exception as e:\n",
    "    print(f\"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}. Google Drive ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "# --- [ â˜…â˜…â˜… (NEW) ë”ë¯¸ ë°ì´í„° ìˆ˜ì • â˜…â˜…â˜… ] ---\n",
    "# (í•µì‹¬ ìš”ì•½ë¬¸: \"ê¹€ë´‰ì§„ ëŒ€í‘œê°€ ë§¤ê°ì„¤ì„ 'ë¶€ì¸'í•˜ê³  'ì•„ì‹œì•„ ë…ì ì§„ì¶œ'ì„ ì„ ì–¸í–ˆë‹¤.\")\n",
    "test_summary_text = \"ê¹€ë´‰ì§„ ìš°ì•„í•œí˜•ì œë“¤ ëŒ€í‘œê°€ ë§¤ê°ì„¤ì„ ì¼ì¶•í•˜ë©°, ì•„ì‹œì•„ 11ê°œêµ­ ì‹œì¥ì— ë…ìì ìœ¼ë¡œ ì§„ì¶œí•˜ê² ë‹¤ëŠ” ì‚¬ì—… ê³„íšì„ ë°í˜”ë‹¤.\"\n",
    "# --- [ â˜…â˜…â˜… ìˆ˜ì • ë â˜…â˜…â˜… ] ---\n",
    "\n",
    "\n",
    "# --- (NEW) ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜ ---\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ 1: [ì •ìƒ] ì˜ë¯¸ê°€ ê±°ì˜ ì¼ì¹˜í•˜ëŠ” Pair (Paraphrase)\n",
    "# (ìš”ì•½ë¬¸ì˜ í•µì‹¬ ë‹¨ì–´(ë¶€ì¸, ë…ì ì§„ì¶œ)ë¥¼ ë™ì˜ì–´ë¡œ í‘œí˜„)\n",
    "test_headline_1 = \"ìš°ì•„í•œí˜•ì œë“¤, 'ë§¤ê°ì„¤ ì‚¬ì‹¤ë¬´ê·¼'â€¦ ê¹€ë´‰ì§„ ëŒ€í‘œ 'ì•„ì‹œì•„ ë…ì ê³µëµ'\"\n",
    "test_summary_1 = test_summary_text\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ 2: [ë‚šì‹œì„±] ì˜ë¯¸ê°€ ì •ë°˜ëŒ€ì¸ Pair (Contradiction)\n",
    "# (ì£¼ì œì–´ëŠ” ê°™ì§€ë§Œ, 'ì¼ì¶•' -> 'ì¸ì •'ìœ¼ë¡œ í•µì‹¬ ì˜ë¯¸ê°€ ì •ë°˜ëŒ€)\n",
    "test_headline_2 = \"ê¹€ë´‰ì§„ ëŒ€í‘œ, ê²°êµ­ ë§¤ê° 'ê³µì‹ ì¸ì •'â€¦ 'M&A ë¶ˆê°€í”¼í–ˆë‹¤'\"\n",
    "test_summary_2 = test_summary_text\n",
    "\n",
    "# (NEW) í…ŒìŠ¤íŠ¸ 3: [ë‚šì‹œì„±] ì£¼ì œëŠ” ë¹„ìŠ·í•˜ë‚˜ í•µì‹¬ ë‚´ìš©ì´ ì—†ëŠ” Pair (Vague Clickbait)\n",
    "# (ë§¤ê°ì„¤, ê¹€ë´‰ì§„ ë“± í‚¤ì›Œë“œëŠ” ìˆìœ¼ë‚˜ 'ì¼ì¶•/ì¸ì •' ì—¬ë¶€ê°€ ì—†ìŒ)\n",
    "test_headline_3 = \"ê¹€ë´‰ì§„ ëŒ€í‘œ 'ì¶©ê²© ì„ ì–¸'... ìˆ˜ì¡°ì›ëŒ€ ë§¤ê°ì„¤ì˜ ì§„ì‹¤ì€?\"\n",
    "test_summary_3 = test_summary_text\n",
    "\n",
    "# (NEW) í…ŒìŠ¤íŠ¸ 4: [ë¬´ê´€] ì£¼ì œê°€ ì™„ì „íˆ ë‹¤ë¥¸ Pair (Unrelated)\n",
    "# (ì£¼ì œì–´ ìì²´ê°€ ì™„ì „íˆ ë‹¤ë¦„)\n",
    "test_headline_4 = \"ì£¼ë§ë¶€í„° ì¥ë§ˆì „ì„  ë¶ìƒ, ì „êµ­ì— í­ìš° ìŸì•„ì§ˆ ë“¯\"\n",
    "test_summary_4 = test_summary_text\n",
    "\n",
    "# --- [ â˜…â˜…â˜… ìˆ˜ì • ë â˜…â˜…â˜… ] ---\n",
    "\n",
    "\n",
    "# --- ê° ë¬¸ì¥ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ ---\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì„ë² ë”© ì¤‘...\")\n",
    "try:\n",
    "    embedding_summary = model.encode(test_summary_text, convert_to_tensor=True) # ìš”ì•½ë¬¸ì€ í•œ ë²ˆë§Œ ì¸ì½”ë”©\n",
    "\n",
    "    embedding1 = model.encode(test_headline_1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(test_headline_2, convert_to_tensor=True)\n",
    "    embedding3 = model.encode(test_headline_3, convert_to_tensor=True)\n",
    "    embedding4 = model.encode(test_headline_4, convert_to_tensor=True) # (NEW) 4ë²ˆì§¸ ì„ë² ë”©\n",
    "\n",
    "\n",
    "    # --- ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ---\n",
    "    cos_sim_1 = util.cos_sim(embedding1, embedding_summary)[0][0]\n",
    "    cos_sim_2 = util.cos_sim(embedding2, embedding_summary)[0][0]\n",
    "    cos_sim_3 = util.cos_sim(embedding3, embedding_summary)[0][0]\n",
    "    cos_sim_4 = util.cos_sim(embedding4, embedding_summary)[0][0] # (NEW) 4ë²ˆì§¸ ìœ ì‚¬ë„\n",
    "\n",
    "\n",
    "    print(\"\\n--- ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼ ---\")\n",
    "    print(f\"[í…ŒìŠ¤íŠ¸ 1: ì •ìƒ (ì˜ë¯¸ ì¼ì¹˜)] (ê¸°ëŒ€ê°’: ë§¤ìš° ë†’ìŒ) \\t ìœ ì‚¬ë„: {cos_sim_1:.4f}\")\n",
    "    print(f\"[í…ŒìŠ¤íŠ¸ 2: ë‚šì‹œì„± (ì˜ë¯¸ ë°˜ëŒ€)] (ê¸°ëŒ€ê°’: ë‚®ìŒ) \\t ìœ ì‚¬ë„: {cos_sim_2:.4f}\")\n",
    "    print(f\"[í…ŒìŠ¤íŠ¸ 3: ë‚šì‹œì„± (ì• ë§¤í•¨)] (ê¸°ëŒ€ê°’: ì¤‘ê°„/ë‚®ìŒ) \\t ìœ ì‚¬ë„: {cos_sim_3:.4f}\")\n",
    "    print(f\"[í…ŒìŠ¤íŠ¸ 4: ë¬´ê´€ (ì£¼ì œ ë‹¤ë¦„)] (ê¸°ëŒ€ê°’: ë§¤ìš° ë‚®ìŒ) \\t ìœ ì‚¬ë„: {cos_sim_4:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"--- ğŸ”´ ì˜¤ë¥˜ ---\")\n",
    "    print(f\"ì„ë² ë”© ë˜ëŠ” ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print(\"ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. (ëŸ°íƒ€ì„ ì¬ì‹œì‘ ì—¬ë¶€ í™•ì¸)\")\n",
    "    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì´í›„ 4.3 ì…€ì—ì„œ NameErrorê°€ ë‚˜ì§€ ì•Šë„ë¡ ì„ì˜ê°’ í• ë‹¹\n",
    "    cos_sim_1, cos_sim_2 = 0.0, 0.0\n",
    "    cos_sim_3 = 0.0\n",
    "    cos_sim_4 = 0.0 # (NEW) 4ë²ˆì§¸ ë³€ìˆ˜ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803fbe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nimport os # (os ì„í¬íŠ¸ ì¶”ê°€)\\n\\n# --- [ â˜…â˜…â˜… ìˆ˜ì • í•„ìš” â˜…â˜…â˜… ] ---\\n# 1. Google Driveì˜ \\'Trained_simcse_model\\' í´ë”ì—ì„œ\\n#    ë¶ˆëŸ¬ì˜¤ê³  ì‹¶ì€ ëª¨ë¸ì˜ í´ë”ëª…(ì˜ˆ: \\'model_251112_1841\\')ì„ ë³µì‚¬í•©ë‹ˆë‹¤.\\nMANUAL_MODEL_NAME = \"model_251113_1109\" # â˜…â˜…â˜… ì—¬ê¸°ì— í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ í´ë”ëª… ì…ë ¥ â˜…â˜…â˜…\\n\\n# 2. (NEW) ìµœì¢… ê²½ë¡œë¥¼ ì™„ì„±í•©ë‹ˆë‹¤. (ë„ì–´ì“°ê¸° ì—†ëŠ” ê²½ë¡œ í™•ì¸)\\nbase_save_dir = \"/content/gdrive/MyDrive/Colab_Notebooks/ssu_NLP/Trained_simcse_model\"\\n\\n# (ì¤‘ìš”) 4.2, 4.3 ì…€ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ë³€ìˆ˜ëª…ì„ \\'model_save_path\\'ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.\\nmodel_save_path = os.path.join(base_save_dir, MANUAL_MODEL_NAME)\\n# --- [ â˜…â˜…â˜… ìˆ˜ì • ë â˜…â˜…â˜… ] ---\\n\\ntry:\\n    print(f\"Google Driveì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤: {model_save_path}\")\\n    model = SentenceTransformer(model_save_path)\\n    print(f\"ì €ì¥ëœ ëª¨ë¸ \\'{model_save_path}\\' ë¡œë“œ ì™„ë£Œ\")\\nexcept Exception as e:\\n    print(f\"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}\")\\n    print(\"ê²½ë¡œì™€ í´ë”ëª…ì´ ì˜¬ë°”ë¥¸ì§€, Google Driveê°€ ë§ˆìš´íŠ¸ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\\n\\n\\n# --- [ â˜…â˜…â˜… (NEW) ë”ë¯¸ ë°ì´í„° ìˆ˜ì • â˜…â˜…â˜… ] ---\\n# (ê¸°ì¤€ì´ ë˜ëŠ” í•µì‹¬ ìš”ì•½ë¬¸)\\ntest_summary_text = \"ê¹€ë´‰ì§„ ìš°ì•„í•œí˜•ì œë“¤ ëŒ€í‘œê°€ ë§¤ê°ì„¤ì„ ì¼ì¶•í•˜ë©°, ì•„ì‹œì•„ 11ê°œêµ­ ì‹œì¥ì— ë…ìì ìœ¼ë¡œ ì§„ì¶œí•˜ê² ë‹¤ëŠ” ì‚¬ì—… ê³„íšì„ ë°í˜”ë‹¤.\"\\n# --- [ â˜…â˜…â˜… ìˆ˜ì • ë â˜…â˜…â˜… ] ---\\n\\n\\n# --- (NEW) ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜ ---\\n\\n# í…ŒìŠ¤íŠ¸ 1: [ì •ìƒ] ì˜ë¯¸ê°€ ê±°ì˜ ì¼ì¹˜í•˜ëŠ” Pair (Paraphrase)\\n# (ìš”ì•½ë¬¸ì˜ í•µì‹¬ ë‹¨ì–´(ë¶€ì¸, ë…ì ì§„ì¶œ)ë¥¼ ë™ì˜ì–´ë¡œ í‘œí˜„)\\ntest_headline_1 = \"ê¹€ë´‰ì§„ ëŒ€í‘œ, \\'ë§¤ê°ì„¤ ì¼ì¶•\\'â€¦ ì•„ì‹œì•„ ë…ì ì§„ì¶œ ì‚¬ì—…ê³„íš ë°œí‘œ\"\\n\\n# í…ŒìŠ¤íŠ¸ 2: [ë‚šì‹œì„±] ì˜ë¯¸ê°€ ì •ë°˜ëŒ€ì¸ Pair (Contradiction)\\n# (ì£¼ì œì–´ëŠ” ê°™ì§€ë§Œ, \\'ì¼ì¶•\\' -> \\'ì¸ì •\\'ìœ¼ë¡œ í•µì‹¬ ì˜ë¯¸ê°€ ì •ë°˜ëŒ€)\\ntest_headline_2 = \"ê¹€ë´‰ì§„ ëŒ€í‘œ, ê²°êµ­ ë§¤ê° \\'ê³µì‹ ì¸ì •\\'â€¦ \\'M&A ë¶ˆê°€í”¼í–ˆë‹¤\\'\"\\n\\n# í…ŒìŠ¤íŠ¸ 3: [ë‚šì‹œì„±] ì£¼ì œëŠ” ë¹„ìŠ·í•˜ë‚˜ í•µì‹¬ ë‚´ìš©ì´ ì—†ëŠ” Pair (Vague Clickbait)\\n# (ë§¤ê°ì„¤, ê¹€ë´‰ì§„ ë“± í‚¤ì›Œë“œëŠ” ìˆìœ¼ë‚˜ \\'ì¼ì¶•/ì¸ì •\\' ì—¬ë¶€ê°€ ì—†ìŒ)\\ntest_headline_3 = \"ê¹€ë´‰ì§„ ëŒ€í‘œ \\'ì¶©ê²© ì„ ì–¸\\'... ìˆ˜ì¡°ì›ëŒ€ ë§¤ê°ì„¤ì˜ ì§„ì‹¤ì€?\"\\n\\n# í…ŒìŠ¤íŠ¸ 4: [ë¬´ê´€] ì£¼ì œê°€ ì™„ì „íˆ ë‹¤ë¥¸ Pair (Unrelated)\\n# (ì£¼ì œì–´ ìì²´ê°€ ì™„ì „íˆ ë‹¤ë¦„)\\ntest_headline_4 = \"ì£¼ë§ë¶€í„° ì¥ë§ˆì „ì„  ë¶ìƒ, ì „êµ­ì— í­ìš° ìŸì•„ì§ˆ ë“¯\"\\n\\n# --- [ â˜…â˜…â˜… ìˆ˜ì • ë â˜…â˜…â˜… ] ---\\n\\n\\n# --- ê° ë¬¸ì¥ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ ---\\nprint(\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì„ë² ë”© ì¤‘...\")\\ntry:\\n    # (NEW) ìš”ì•½ë¬¸ì€ í•œ ë²ˆë§Œ ì¸ì½”ë”©í•˜ì—¬ ì¬ì‚¬ìš©\\n    embedding_summary = model.encode(test_summary_text, convert_to_tensor=True)\\n\\n    embedding1 = model.encode(test_headline_1, convert_to_tensor=True)\\n    embedding2 = model.encode(test_headline_2, convert_to_tensor=True)\\n    embedding3 = model.encode(test_headline_3, convert_to_tensor=True)\\n    embedding4 = model.encode(test_headline_4, convert_to_tensor=True)\\n\\n\\n    # --- ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ---\\n    cos_sim_1 = util.cos_sim(embedding1, embedding_summary)[0][0]\\n    cos_sim_2 = util.cos_sim(embedding2, embedding_summary)[0][0]\\n    cos_sim_3 = util.cos_sim(embedding3, embedding_summary)[0][0]\\n    cos_sim_4 = util.cos_sim(embedding4, embedding_summary)[0][0]\\n\\n\\n    print(\"\\n--- ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼ ---\")\\n    print(f\"[í…ŒìŠ¤íŠ¸ 1: ì •ìƒ (ì˜ë¯¸ ì¼ì¹˜)] (ê¸°ëŒ€ê°’: ë§¤ìš° ë†’ìŒ) \\t ìœ ì‚¬ë„: {cos_sim_1:.4f}\")\\n    print(f\"[í…ŒìŠ¤íŠ¸ 2: ë‚šì‹œì„± (ì˜ë¯¸ ë°˜ëŒ€)] (ê¸°ëŒ€ê°’: ë‚®ìŒ) \\t ìœ ì‚¬ë„: {cos_sim_2:.4f}\")\\n    print(f\"[í…ŒìŠ¤íŠ¸ 3: ë‚šì‹œì„± (ì• ë§¤í•¨)] (ê¸°ëŒ€ê°’: ì¤‘ê°„/ë‚®ìŒ) \\t ìœ ì‚¬ë„: {cos_sim_3:.4f}\")\\n    print(f\"[í…ŒìŠ¤íŠ¸ 4: ë¬´ê´€ (ì£¼ì œ ë‹¤ë¦„)] (ê¸°ëŒ€ê°’: ë§¤ìš° ë‚®ìŒ) \\t ìœ ì‚¬ë„: {cos_sim_4:.4f}\")\\n\\nexcept Exception as e:\\n    print(f\"--- ğŸ”´ ì˜¤ë¥˜ ---\")\\n    print(f\"ì„ë² ë”© ë˜ëŠ” ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\\n    print(\"ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\\n    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì´í›„ 4.3 ì…€ì—ì„œ NameErrorê°€ ë‚˜ì§€ ì•Šë„ë¡ ì„ì˜ê°’ í• ë‹¹\\n    cos_sim_1, cos_sim_2 = 0.0, 0.0\\n    cos_sim_3, cos_sim_4 = 0.0, 0.0\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-- 4.1. í•™ìŠµëœ ëª¨ë¸ë¡œ ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸ (â˜…ëŸ°íƒ€ì„ ì¬ì‹œì‘ ì‹œ ìˆ˜ë™ ë¡œë“œâ˜…)\n",
    "'''\n",
    "import os # (os ì„í¬íŠ¸ ì¶”ê°€)\n",
    "\n",
    "# --- [ â˜…â˜…â˜… ìˆ˜ì • í•„ìš” â˜…â˜…â˜… ] ---\n",
    "# 1. Google Driveì˜ 'Trained_simcse_model' í´ë”ì—ì„œ\n",
    "#    ë¶ˆëŸ¬ì˜¤ê³  ì‹¶ì€ ëª¨ë¸ì˜ í´ë”ëª…(ì˜ˆ: 'model_251112_1841')ì„ ë³µì‚¬í•©ë‹ˆë‹¤.\n",
    "MANUAL_MODEL_NAME = \"model_251113_1109\" # â˜…â˜…â˜… ì—¬ê¸°ì— í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ í´ë”ëª… ì…ë ¥ â˜…â˜…â˜…\n",
    "\n",
    "# 2. (NEW) ìµœì¢… ê²½ë¡œë¥¼ ì™„ì„±í•©ë‹ˆë‹¤. (ë„ì–´ì“°ê¸° ì—†ëŠ” ê²½ë¡œ í™•ì¸)\n",
    "base_save_dir = \"/content/gdrive/MyDrive/Colab_Notebooks/ssu_NLP/Trained_simcse_model\"\n",
    "\n",
    "# (ì¤‘ìš”) 4.2, 4.3 ì…€ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ë³€ìˆ˜ëª…ì„ 'model_save_path'ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "model_save_path = os.path.join(base_save_dir, MANUAL_MODEL_NAME)\n",
    "# --- [ â˜…â˜…â˜… ìˆ˜ì • ë â˜…â˜…â˜… ] ---\n",
    "\n",
    "try:\n",
    "    print(f\"Google Driveì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤: {model_save_path}\")\n",
    "    model = SentenceTransformer(model_save_path)\n",
    "    print(f\"ì €ì¥ëœ ëª¨ë¸ '{model_save_path}' ë¡œë“œ ì™„ë£Œ\")\n",
    "except Exception as e:\n",
    "    print(f\"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ê²½ë¡œì™€ í´ë”ëª…ì´ ì˜¬ë°”ë¥¸ì§€, Google Driveê°€ ë§ˆìš´íŠ¸ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "# --- [ â˜…â˜…â˜… (NEW) ë”ë¯¸ ë°ì´í„° ìˆ˜ì • â˜…â˜…â˜… ] ---\n",
    "# (ê¸°ì¤€ì´ ë˜ëŠ” í•µì‹¬ ìš”ì•½ë¬¸)\n",
    "test_summary_text = \"ê¹€ë´‰ì§„ ìš°ì•„í•œí˜•ì œë“¤ ëŒ€í‘œê°€ ë§¤ê°ì„¤ì„ ì¼ì¶•í•˜ë©°, ì•„ì‹œì•„ 11ê°œêµ­ ì‹œì¥ì— ë…ìì ìœ¼ë¡œ ì§„ì¶œí•˜ê² ë‹¤ëŠ” ì‚¬ì—… ê³„íšì„ ë°í˜”ë‹¤.\"\n",
    "# --- [ â˜…â˜…â˜… ìˆ˜ì • ë â˜…â˜…â˜… ] ---\n",
    "\n",
    "\n",
    "# --- (NEW) ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜ ---\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ 1: [ì •ìƒ] ì˜ë¯¸ê°€ ê±°ì˜ ì¼ì¹˜í•˜ëŠ” Pair (Paraphrase)\n",
    "# (ìš”ì•½ë¬¸ì˜ í•µì‹¬ ë‹¨ì–´(ë¶€ì¸, ë…ì ì§„ì¶œ)ë¥¼ ë™ì˜ì–´ë¡œ í‘œí˜„)\n",
    "test_headline_1 = \"ê¹€ë´‰ì§„ ëŒ€í‘œ, 'ë§¤ê°ì„¤ ì¼ì¶•'â€¦ ì•„ì‹œì•„ ë…ì ì§„ì¶œ ì‚¬ì—…ê³„íš ë°œí‘œ\"\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ 2: [ë‚šì‹œì„±] ì˜ë¯¸ê°€ ì •ë°˜ëŒ€ì¸ Pair (Contradiction)\n",
    "# (ì£¼ì œì–´ëŠ” ê°™ì§€ë§Œ, 'ì¼ì¶•' -> 'ì¸ì •'ìœ¼ë¡œ í•µì‹¬ ì˜ë¯¸ê°€ ì •ë°˜ëŒ€)\n",
    "test_headline_2 = \"ê¹€ë´‰ì§„ ëŒ€í‘œ, ê²°êµ­ ë§¤ê° 'ê³µì‹ ì¸ì •'â€¦ 'M&A ë¶ˆê°€í”¼í–ˆë‹¤'\"\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ 3: [ë‚šì‹œì„±] ì£¼ì œëŠ” ë¹„ìŠ·í•˜ë‚˜ í•µì‹¬ ë‚´ìš©ì´ ì—†ëŠ” Pair (Vague Clickbait)\n",
    "# (ë§¤ê°ì„¤, ê¹€ë´‰ì§„ ë“± í‚¤ì›Œë“œëŠ” ìˆìœ¼ë‚˜ 'ì¼ì¶•/ì¸ì •' ì—¬ë¶€ê°€ ì—†ìŒ)\n",
    "test_headline_3 = \"ê¹€ë´‰ì§„ ëŒ€í‘œ 'ì¶©ê²© ì„ ì–¸'... ìˆ˜ì¡°ì›ëŒ€ ë§¤ê°ì„¤ì˜ ì§„ì‹¤ì€?\"\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ 4: [ë¬´ê´€] ì£¼ì œê°€ ì™„ì „íˆ ë‹¤ë¥¸ Pair (Unrelated)\n",
    "# (ì£¼ì œì–´ ìì²´ê°€ ì™„ì „íˆ ë‹¤ë¦„)\n",
    "test_headline_4 = \"ì£¼ë§ë¶€í„° ì¥ë§ˆì „ì„  ë¶ìƒ, ì „êµ­ì— í­ìš° ìŸì•„ì§ˆ ë“¯\"\n",
    "\n",
    "# --- [ â˜…â˜…â˜… ìˆ˜ì • ë â˜…â˜…â˜… ] ---\n",
    "\n",
    "\n",
    "# --- ê° ë¬¸ì¥ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ ---\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì„ë² ë”© ì¤‘...\")\n",
    "try:\n",
    "    # (NEW) ìš”ì•½ë¬¸ì€ í•œ ë²ˆë§Œ ì¸ì½”ë”©í•˜ì—¬ ì¬ì‚¬ìš©\n",
    "    embedding_summary = model.encode(test_summary_text, convert_to_tensor=True)\n",
    "\n",
    "    embedding1 = model.encode(test_headline_1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(test_headline_2, convert_to_tensor=True)\n",
    "    embedding3 = model.encode(test_headline_3, convert_to_tensor=True)\n",
    "    embedding4 = model.encode(test_headline_4, convert_to_tensor=True)\n",
    "\n",
    "\n",
    "    # --- ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ---\n",
    "    cos_sim_1 = util.cos_sim(embedding1, embedding_summary)[0][0]\n",
    "    cos_sim_2 = util.cos_sim(embedding2, embedding_summary)[0][0]\n",
    "    cos_sim_3 = util.cos_sim(embedding3, embedding_summary)[0][0]\n",
    "    cos_sim_4 = util.cos_sim(embedding4, embedding_summary)[0][0]\n",
    "\n",
    "\n",
    "    print(\"\\n--- ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼ ---\")\n",
    "    print(f\"[í…ŒìŠ¤íŠ¸ 1: ì •ìƒ (ì˜ë¯¸ ì¼ì¹˜)] (ê¸°ëŒ€ê°’: ë§¤ìš° ë†’ìŒ) \\t ìœ ì‚¬ë„: {cos_sim_1:.4f}\")\n",
    "    print(f\"[í…ŒìŠ¤íŠ¸ 2: ë‚šì‹œì„± (ì˜ë¯¸ ë°˜ëŒ€)] (ê¸°ëŒ€ê°’: ë‚®ìŒ) \\t ìœ ì‚¬ë„: {cos_sim_2:.4f}\")\n",
    "    print(f\"[í…ŒìŠ¤íŠ¸ 3: ë‚šì‹œì„± (ì• ë§¤í•¨)] (ê¸°ëŒ€ê°’: ì¤‘ê°„/ë‚®ìŒ) \\t ìœ ì‚¬ë„: {cos_sim_3:.4f}\")\n",
    "    print(f\"[í…ŒìŠ¤íŠ¸ 4: ë¬´ê´€ (ì£¼ì œ ë‹¤ë¦„)] (ê¸°ëŒ€ê°’: ë§¤ìš° ë‚®ìŒ) \\t ìœ ì‚¬ë„: {cos_sim_4:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"--- ğŸ”´ ì˜¤ë¥˜ ---\")\n",
    "    print(f\"ì„ë² ë”© ë˜ëŠ” ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print(\"ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì´í›„ 4.3 ì…€ì—ì„œ NameErrorê°€ ë‚˜ì§€ ì•Šë„ë¡ ì„ì˜ê°’ í• ë‹¹\n",
    "    cos_sim_1, cos_sim_2 = 0.0, 0.0\n",
    "    cos_sim_3, cos_sim_4 = 0.0, 0.0\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1477e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. 3001ê°œì˜ ì‹¤ì œ ê²€ì¦(validation) ë°ì´í„°ë¡œ ì„ê³„ê°’ íƒìƒ‰ ---\n",
      "--- 2. ì„ë² ë”© ë° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘... (ë°ì´í„°ê°€ ë§ìœ¼ë©´ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92fdf9e2e62456aa7ef2f46a398c57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee6f47f17294e9b8eb569d130404cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ. (ìƒ˜í”Œ 5ê°œ: [0.665  0.6133 0.9579 0.9979 0.9   ])\n",
      "--- 3. ìµœì ì˜ F1-Score ê¸°ì¤€ ì„ê³„ê°’ íƒìƒ‰ ì¤‘... ---\n",
      "\n",
      "--- 4. íƒìƒ‰ ê²°ê³¼ (ê²€ì¦ ë°ì´í„° 10%) ---\n",
      "  ìµœì ì˜ F1-Score: 0.8472\n",
      "  ë‹¹ì‹œì˜ Accuracy : 0.7927\n",
      "  ìµœì ì˜ ì„ê³„ê°’(Threshold): 0.73\n",
      "\n",
      "(ì´ ì„ê³„ê°’ì„ 5ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°ì— ì ìš©í•©ë‹ˆë‹¤.)\n"
     ]
    }
   ],
   "source": [
    "# 4.2. ìµœì ì˜ ì„ê³„ê°’(Threshold) ì°¾ê¸° (ì‹¤ì œ ê²€ì¦ ë°ì´í„° ì‚¬ìš©)\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. ì‹¤ì œ í‰ê°€(Test) ë°ì´í„°ì…‹ ì¤€ë¹„ ---\n",
    "# (NEW) 8ë²ˆ ì…€(3.3)ì—ì„œ ë¶„ë¦¬í•œ 10%ì˜ 'validation_data_list'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "if 'validation_data_list' in locals() and validation_data_list:\n",
    "    print(f\"--- 1. {len(validation_data_list)}ê°œì˜ ì‹¤ì œ ê²€ì¦(validation) ë°ì´í„°ë¡œ ì„ê³„ê°’ íƒìƒ‰ ---\")\n",
    "\n",
    "    # (NEW) ë°ì´í„°ë¥¼ test_pairsì™€ ground_truth_labelsë¡œ ë³€í™˜\n",
    "    test_pairs = [(data[0], data[1]) for data in validation_data_list]\n",
    "    ground_truth_labels = [data[2] for data in validation_data_list]\n",
    "\n",
    "else:\n",
    "    print(\"--- 1. ğŸ”´ ê²½ê³ : 'validation_data_list'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (8ë²ˆ ì…€ ì‹¤í–‰ í•„ìš”) ---\")\n",
    "    print(\"--- ì„ì‹œ ë”ë¯¸ ë°ì´í„°ë¡œ ëŒ€ì‹  ì‹¤í–‰í•©ë‹ˆë‹¤. ---\")\n",
    "    test_pairs = [\n",
    "        (\"ê¹€ë´‰ì§„, 40ì–µë‹¬ëŸ¬ ë”œë¡œ ì•„ì‹œì•„ 11ê°œêµ­ ì´ê´„\", \"ìš°ì•„í•œí˜•ì œë“¤ ê¹€ë´‰ì§„ ëŒ€í‘œê°€...\"),\n",
    "        (\"ì„œìš¸ ì•„íŒŒíŠ¸ ê±°ë˜ëŸ‰ 3ê°œì›” ì—°ì† ê°ì†Œ\", \"ê¸ˆë¦¬ ì¸ìƒê³¼ ëŒ€ì¶œ ê·œì œì˜ ì˜í–¥ìœ¼ë¡œ...\"),\n",
    "        (\"ê¹€ë´‰ì§„ ëŒ€í‘œ, íšŒì‚¬ ë§¤ê° í›„ ëŒì—° ì ì \", \"ìš°ì•„í•œí˜•ì œë“¤ ê¹€ë´‰ì§„ ëŒ€í‘œê°€...\"),\n",
    "        (\"ì„œìš¸ ì•„íŒŒíŠ¸ 'í­ë½' ì‹œì‘ëë‹¤!\", \"ê¸ˆë¦¬ ì¸ìƒê³¼ ëŒ€ì¶œ ê·œì œì˜ ì˜í–¥ìœ¼ë¡œ...\")\n",
    "    ]\n",
    "    ground_truth_labels = [1, 1, 0, 0]\n",
    "\n",
    "\n",
    "# --- 2. ëª¨ë¸ë¡œ ìœ ì‚¬ë„ ê³„ì‚° ---\n",
    "# (model ë³€ìˆ˜ëŠ” 3.7ì—ì„œ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©)\n",
    "# (6ë²ˆ ì…€ì—ì„œ ì„í¬íŠ¸í•œ 'util' (from sentence_transformers import util)ì„ ì‚¬ìš©)\n",
    "print(\"--- 2. ì„ë² ë”© ë° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘... (ë°ì´í„°ê°€ ë§ìœ¼ë©´ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤) ---\")\n",
    "\n",
    "# (NEW) ë°°ì¹˜ ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ì—¬ ì†ë„ í–¥ìƒ\n",
    "test_embeddings1 = model.encode([pair[0] for pair in test_pairs], convert_to_tensor=True, show_progress_bar=True)\n",
    "test_embeddings2 = model.encode([pair[1] for pair in test_pairs], convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "similarities = []\n",
    "for emb1, emb2 in zip(test_embeddings1, test_embeddings2):\n",
    "    sim = util.cos_sim(emb1, emb2)\n",
    "    similarities.append(sim.item())\n",
    "\n",
    "print(f\"ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ. (ìƒ˜í”Œ 5ê°œ: {np.round(similarities[:5], 4)})\")\n",
    "\n",
    "# --- 3. ìµœì ì˜ ì„ê³„ê°’ íƒìƒ‰ (F1-Score ê¸°ì¤€) ---\n",
    "\n",
    "print(\"--- 3. ìµœì ì˜ F1-Score ê¸°ì¤€ ì„ê³„ê°’ íƒìƒ‰ ì¤‘... ---\")\n",
    "\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "best_acc = 0 # (NEW) F1ì´ ìµœëŒ€ì¼ ë•Œì˜ ì •í™•ë„\n",
    "\n",
    "# 0.01ë¶€í„° 1.00ê¹Œì§€ 100ê°œì˜ ì„ê³„ê°’ í›„ë³´ë¥¼ í…ŒìŠ¤íŠ¸\n",
    "for threshold_candidate in [i * 0.01 for i in range(1, 101)]:\n",
    "    # í˜„ì¬ ì„ê³„ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ (ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ë³´ë‹¤ í¬ë©´ 1, ì•„ë‹ˆë©´ 0)\n",
    "    preds = [1 if s > threshold_candidate else 0 for s in similarities]\n",
    "\n",
    "    # F1-Score ê³„ì‚°\n",
    "    f1 = f1_score(ground_truth_labels, preds, pos_label=1, zero_division=0)\n",
    "\n",
    "    # ìµœê³  F1-Score ê°±ì‹ \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold_candidate\n",
    "        best_acc = accuracy_score(ground_truth_labels, preds) # (NEW)\n",
    "\n",
    "print(\"\\n--- 4. íƒìƒ‰ ê²°ê³¼ (ê²€ì¦ ë°ì´í„° 10%) ---\")\n",
    "print(f\"  ìµœì ì˜ F1-Score: {best_f1:.4f}\")\n",
    "print(f\"  ë‹¹ì‹œì˜ Accuracy : {best_acc:.4f}\")\n",
    "print(f\"  ìµœì ì˜ ì„ê³„ê°’(Threshold): {best_threshold:.2f}\")\n",
    "print(\"\\n(ì´ ì„ê³„ê°’ì„ 5ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°ì— ì ìš©í•©ë‹ˆë‹¤.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2102d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.2ì—ì„œ ì°¾ì€ ìµœì  ì„ê³„ê°’(Threshold) 0.73 ì ìš© ---\n",
      "\n",
      "(4.1 ì…€ì˜ ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸)\n",
      "[í…ŒìŠ¤íŠ¸ 1: ì •ìƒ (ì˜ë¯¸ ì¼ì¹˜)] (ìœ ì‚¬ë„ 0.9604) \t íŒì •: [ì •ìƒ]\n",
      "[í…ŒìŠ¤íŠ¸ 2: ë‚šì‹œì„± (ì˜ë¯¸ ë°˜ëŒ€)] (ìœ ì‚¬ë„ 0.9459) \t íŒì •: [ì •ìƒ]\n",
      "[í…ŒìŠ¤íŠ¸ 3: ë‚šì‹œì„± (ì• ë§¤í•¨)] (ìœ ì‚¬ë„ 0.9402) \t íŒì •: [ì •ìƒ]\n",
      "[í…ŒìŠ¤íŠ¸ 4: ë¬´ê´€ (ì£¼ì œ ë‹¤ë¦„)] (ìœ ì‚¬ë„ 0.8916) \t íŒì •: [ì •ìƒ]\n"
     ]
    }
   ],
   "source": [
    "# 4.3. (ìˆ˜ë™ í…ŒìŠ¤íŠ¸) ê³„ì‚°ëœ ìµœì  ì„ê³„ê°’ ì ìš©\n",
    "\n",
    "# (NEW) 4.2 ì…€ì—ì„œ ê³„ì‚°ëœ 'best_threshold' ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "try:\n",
    "    THRESHOLD = best_threshold\n",
    "    print(f\"--- 4.2ì—ì„œ ì°¾ì€ ìµœì  ì„ê³„ê°’(Threshold) {THRESHOLD:.2f} ì ìš© ---\")\n",
    "except NameError:\n",
    "    print(\"--- ğŸ”´ ì˜¤ë¥˜: 'best_threshold' ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. (4.2 ì…€ ì‹¤í–‰ í•„ìš”) ---\")\n",
    "    print(\"--- ì„ì‹œ ì„ê³„ê°’ 0.65ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ---\")\n",
    "    THRESHOLD = 0.65\n",
    "\n",
    "# (4.1 ì…€ì˜ 'cos_sim_1' ~ 'cos_sim_4' ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤)\n",
    "# (4.1 ì…€ì„ ì‹¤í–‰í•´ì•¼ ì´ ë³€ìˆ˜ë“¤ì´ ì¡´ì¬í•©ë‹ˆë‹¤)\n",
    "try:\n",
    "    print(\"\\n(4.1 ì…€ì˜ ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸)\")\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ 1 íŒì •\n",
    "    judgment_1 = '[ì •ìƒ]' if cos_sim_1 > THRESHOLD else '[ë‚šì‹œì„±]'\n",
    "    print(f\"[í…ŒìŠ¤íŠ¸ 1: ì •ìƒ (ì˜ë¯¸ ì¼ì¹˜)] (ìœ ì‚¬ë„ {cos_sim_1:.4f}) \\t íŒì •: {judgment_1}\")\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ 2 íŒì •\n",
    "    judgment_2 = '[ì •ìƒ]' if cos_sim_2 > THRESHOLD else '[ë‚šì‹œì„±]'\n",
    "    print(f\"[í…ŒìŠ¤íŠ¸ 2: ë‚šì‹œì„± (ì˜ë¯¸ ë°˜ëŒ€)] (ìœ ì‚¬ë„ {cos_sim_2:.4f}) \\t íŒì •: {judgment_2}\")\n",
    "\n",
    "    # (NEW) í…ŒìŠ¤íŠ¸ 3 íŒì •\n",
    "    judgment_3 = '[ì •ìƒ]' if cos_sim_3 > THRESHOLD else '[ë‚šì‹œì„±]'\n",
    "    print(f\"[í…ŒìŠ¤íŠ¸ 3: ë‚šì‹œì„± (ì• ë§¤í•¨)] (ìœ ì‚¬ë„ {cos_sim_3:.4f}) \\t íŒì •: {judgment_3}\")\n",
    "\n",
    "    # (NEW) í…ŒìŠ¤íŠ¸ 4 íŒì •\n",
    "    judgment_4 = '[ì •ìƒ]' if cos_sim_4 > THRESHOLD else '[ë‚šì‹œì„±]'\n",
    "    print(f\"[í…ŒìŠ¤íŠ¸ 4: ë¬´ê´€ (ì£¼ì œ ë‹¤ë¦„)] (ìœ ì‚¬ë„ {cos_sim_4:.4f}) \\t íŒì •: {judgment_4}\")\n",
    "\n",
    "except NameError:\n",
    "     print(\"\\n--- ğŸ”´ ê²½ê³ : 'cos_sim_...' ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. (4.1 ì…€ ì‹¤í–‰ í•„ìš”) ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d9d5d1",
   "metadata": {},
   "source": [
    "## 5ë‹¨ê³„: ìµœì¢… ì›Œí¬í”Œë¡œìš° ì ìš© (ê°œë…)\n",
    "ì´ì œ ì™„ì„±ëœ ë‘ ëª¨ë¸(KoBART, KoSimCSE)ì„ ì‚¬ìš©ìì˜ ìµœì¢… ì›Œí¬í”Œë¡œìš° 2.Cì— ì ìš©í•©ë‹ˆë‹¤.\n",
    "1. ì…ë ¥: ìƒˆë¡œìš´ ê¸°ì‚¬ [í—¤ë“œë¼ì¸ + ë³¸ë¬¸]ì´ ì…ë ¥ë©ë‹ˆë‹¤.\n",
    "2. ìš”ì•½: [ë³¸ë¬¸]ì„ íŒŒì¸íŠœë‹ëœ KoBART ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ [ìƒì„±ëœ ìš”ì•½ë¬¸]ì„ ì–»ìŠµë‹ˆë‹¤.\n",
    "3. ì„ë² ë”©: [í—¤ë“œë¼ì¸]ê³¼ [ìƒì„±ëœ ìš”ì•½ë¬¸]ì„ íŒŒì¸íŠœë‹ëœ KoSimCSE ëª¨ë¸(model.encode)ì— ë„£ì–´ ë‘ ê°œì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "4. ìœ ì‚¬ë„ ê³„ì‚°: ë‘ ë²¡í„°ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„(util.cos_sim)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "5. íŒë³„: ì´ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ 4ë‹¨ê³„ì—ì„œ ì°¾ì€ **ì„ê³„ê°’(THRESHOLD)**ê³¼ ë¹„êµí•˜ì—¬ 'ì •ìƒ'/'ë‚šì‹œì„±' ì—¬ë¶€ë¥¼ íŒë³„í•©ë‹ˆë‹¤.\n",
    "6. ì—…ë°ì´íŠ¸: íŒë³„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ì–¸ë¡ ì‚¬ì˜ ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aed889d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
